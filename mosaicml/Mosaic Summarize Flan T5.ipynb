{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "759d9cd6-228d-4477-bd3e-5193ed26c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f7703f4-619e-47c7-a283-a9fd7bf02930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc85072-945d-4ce5-a032-f82f32509b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 203037\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 6436\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 6440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db=load_dataset(\"scientific_papers\",\"arxiv\")\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4ca257-0848-4564-9fdc-a8023da286fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['train']=db['train'].shuffle(seed=42).select(range(10000))\n",
    "db['validation']=db['validation'].shuffle(seed=42).select(range(3000))\n",
    "db['test']=db['test'].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e70c93-b0ae-41de-8b6d-12d245a1d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='google/flan-t5-small'\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "model=AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3149797-1c2d-4b71-adc7-f7ef18673039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "data_collator = transformers.data.data_collator.default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9672979-1937-480d-a249-c47390b66df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73920f715eab4701b2eeb794298e3cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def preprocess_data(data):\n",
    "    model_inputs=tokenizer([\"summarize\"+article for article in data['article']],max_length=256,padding='max_length',truncation=True)\n",
    "    labels=tokenizer(data['abstract'],max_length=128,padding='max_length',truncation=True)\n",
    "    model_inputs['labels']=labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "db_train=db['train'].map(preprocess_data,batched=True,remove_columns=['article','abstract','section_names'])\n",
    "db_validation=db['validation'].map(preprocess_data,batched=True,remove_columns=['article','abstract','section_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47695bee-f279-4aad-becf-41591ee3d3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edb42e54-e9f3-445a-95ba-4db6eb0ff76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.data.data_collator.default_data_collator\n",
    "train_loader = DataLoader(db_train, batch_size=16, shuffle=False, drop_last=False, collate_fn=data_collator)\n",
    "eval_loader = DataLoader(db_validation,batch_size=16, shuffle=False, drop_last=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feae1ab4-159a-4362-a648-65c29cc16e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#Convert to composer model\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "from composer.metrics import LanguageCrossEntropy\n",
    "\n",
    "metrics = [LanguageCrossEntropy()]\n",
    "# Package as a trainer-friendly Composer model\n",
    "composer_model = HuggingFaceModel(model, tokenizer=tokenizer, metrics=metrics,use_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad8e5383-c427-49f8-b22f-2c4e5951f7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0087986-d951-427e-90a9-e5da0ce2a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "optimizer = AdamW(\n",
    "    params=composer_model.parameters(),\n",
    "    lr=3e-5, betas=(0.9, 0.98),\n",
    "    eps=1e-6, weight_decay=3e-6\n",
    ")\n",
    "linear_lr_decay = LinearLR(\n",
    "    optimizer, start_factor=1.0,\n",
    "    end_factor=0, total_iters=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0153e16f-4f24-4209-902c-306861e71f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "******************************\n",
      "Config:\n",
      "composer_commit_hash: None\n",
      "composer_version: 0.17.2\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 17\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38ea64c56f44c9cb221f85efb443e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/150 [00:00<?, ?ba/s]                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf2eef23d9f4a0d867343a73227925e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   0:    0%|| 0/188 [00:00<?, ?ba/s]                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwe/newgalaxie/lib/python3.8/site-packages/composer/core/data_spec.py:35: UserWarning: Cannot split tensor of length 8 into batches of size 16. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.\n",
      "  warnings.warn(f'Cannot split tensor of length {len(t)} into batches of size {microbatch_size}. '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from composer import Trainer\n",
    "\n",
    "# Create Trainer Object\n",
    "trainer = Trainer(\n",
    "    model=composer_model, # This is the model from the HuggingFaceModel wrapper class.\n",
    "    train_dataloader=train_loader,\n",
    "    eval_dataloader=eval_loader,\n",
    "    max_duration=\"1ep\",\n",
    "    optimizers=optimizer,\n",
    "    schedulers=[linear_lr_decay],\n",
    "    device='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    train_subset_num_batches=150,\n",
    "    precision='fp32',\n",
    "    seed=17,\n",
    "    \n",
    ")\n",
    "# Start training\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "594ec7fd-1401-465d-9324-6c86ad9d9936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval': {'LanguageCrossEntropy': LanguageCrossEntropy(\n",
       "    (loss_fn): CrossEntropyLoss()\n",
       "  )}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c594399-eba2-4595-9dac-32067db1d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch = next(iter(eval_loader))\n",
    "\n",
    "# Move batch to gpu\n",
    "eval_batch = {k: v.cuda() if torch.cuda.is_available() else v for k, v in eval_batch.items()}\n",
    "with torch.no_grad():\n",
    "    predictions = composer_model(eval_batch)[\"logits\"].argmax(dim=1)\n",
    "\n",
    "# Visualize only 5 samples\n",
    "predictions = predictions[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a6f8bb8-6b1c-4964-8190-e68883cd4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer.state.model.state_dict(), 'mosaic_summarize.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839a843-1cd9-49f0-a309-5252582e36d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6bd04de-159c-47c2-91ff-5ec680c96c6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmosaic_summarize.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnewmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(loaded_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'newmodel' is not defined"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.load('mosaic_summarize.pt')\n",
    "newmodel.load_state_dict(loaded_model['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6e17c87-5570-49c3-a965-50504fed015c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'markov chain monte carlo ( mcmc ) has become a standard tool in bayesian analysis .\\nthe greatest benefit of mcmc is its generality  it is guaranteed to be consistent with virtually no assumptions on the underlying model . however , the practical applicability of mcmc generally depends on the dimension of the unknown variables , the number of data , and the computational resources available .\\nfurthermore , because mcmc is sequential in nature , it can be difficult to implement efficiently with modern parallel and distributed computing architectures ; see @xcite for general discussion about mcmc in challenging scenarios , and with parallel computing architectures .\\nin recent years , a number of generic approximation methods have been developed for complex bayesian inference , for instance variational bayes ( cf .\\nreviews * ? ? ?\\n* ; * ? ? ?\\n* ) , expectation propagation @xcite and laplace approximations @xcite .\\nthese methods have been reported to provide accurate enough inference for practical purposes in many scenarios , with a fraction of computing time compared with ( asymptotically ) exact approaches such as mcmc .\\none problem with approximate methods is that the related bias is typically hard or impossible to quantify , and in practice the only way to ensure the validity of the method is to compare the outcome with an exact method , such as mcmc .\\nwe promote , in broad terms , a well - known principle : the combination of an approximate method for ` preliminary \\' inference , and importance sampling type correction of the output .\\nour primary focus is on the approximation of a marginal distribution ( or marginal likelihood ) and the use of mcmc for approximate marginal inference .\\nthe outcome of the mcmc is corrected with importance sampling type estimates .\\nwe detail minimal requirements which lead to strongly consistent estimators and give general conditions under which central limit theorems hold .\\nour setting highlights explicitly the connection of importance sampling type correction and recently developed pseudo - marginal type mcmc @xcite , such as particle mcmc @xcite , grouped independence metropolis - hastings ( gimh ) @xcite , approximate bayesian computation mcmc @xcite , the algorithm for estimation of discretely observed diffusions suggested in @xcite , and using annealed importance sampling @xcite .\\nit also enables the use of debiased estimators as suggested in @xcite .\\nall these methods are often computationally expensive , but our approach is based on independent realisations of such importance sampling type estimates , which can be calculated efficiently in parallel .\\nwe focus on bayesian state space models , where importance sampling and particle filters are used for correction .\\nwe believe that the setup is useful much more generally , for instance in latent variable models or discretisations of continuous models , such as diffusions or inverse problems .\\nimportance sampling correction of mcmc has been suggested early in the mcmc literature ; see at least @xcite .\\nit has been used , for instance , when estimating bayes factors using a single mcmc output @xcite .\\nbhattacharya @xcite has suggested a consistent estimator based on regeneration and tan , doss and hobert @xcite give such estimators in case of using multiple markov chains .    using unbiased estimators of importance weights in this context\\nhas been suggested at least in @xcite , who consider a generalisation of the pseudo - marginal method , allowing for likelihood estimators that occasionally take negative values .\\nthis allows , for instance , using the recently discovered ` debiasing tricks \\' @xcite in order to construct an unbiased estimator .\\nsimilar tricks , which would lead into non - negative unbiased estimators , as required by the pseudo - marginal method , may be unavailable ( cf . * ? ? ?\\nquiroz , villani and kohn @xcite applied the is correction in the data sub - sampling context .    nested sampling has also appeared in many forms in the monte carlo literature .\\nthe smc^2^ algorithm @xcite is based on an application of nested sequential monte carlo steps , which has similarities with our framework , and the is^2^ method @xcite focuses on the case where the preliminary inference is based on importance sampling .\\nwe focus on the mcmc approximation of the marginal distribution , which we believe often to be easily implementable in practice .\\nthe markov chain nature of the marginal monte carlo approximation comes also with some extra theoretical issues which we address .\\nanother specific feature of our work is that we connect is type correction explicitly with recently discovered ` exact approximation mcmc \\' techniques @xcite .\\ntheoretical advances @xcite have already led to more efficient implementation of such methods , but has also revealed fundamental limitations .\\nfor instance , the methods may suffer from slow ( non - geometric ) convergence in practically interesting scenarios @xcite .\\nrecently proposed correlated version of the pseudo - marginal mcmc @xcite may help in more efficient implementation of pseudo - marginal type methods , but the question of their efficient parallelisability remains a challenge .\\nthe blocked parallelisable particle gibbs @xcite has appealing limiting properties , but its implementation still requires synchronisation between every update cycle , which may be costly in some computing environments .\\nwe recognise that our is approach comes with its own limitations ( see section [ sec : discussion ] ) , but we believe that it can often be very useful with parallel computing facilities .\\nafter preliminaries and notation in section [ sec : notation ] , we discuss a general importance sampling ( is ) type correction of mcmc outputs and we state related consistency results in section [ sec : simple ] .\\nwe detail the general case where unbiased estimators are used ( proposition [ prop : simple - is ] ) and then extend the setting in order to allow inference in an extended state space ( proposition [ prop : proper - consistency ] ) .\\nthe latter result is based on a general concept ( assumption [ a : proper ] ) , which we call ` proper weighting scheme \\' following the terminology of liu @xcite .\\nit accommodates naturally estimators from importance sampling and sequential monte carlo .\\nwe believe that assumption [ a : proper ] , or the more general assumption [ a : super - general ] , could be satisfied relatively easily in a wide variety of scenarios .    in section [ sec : ssm ] ,\\nwe discuss how our importance sampling correction method applies in general state space models ( ssm ) , and observe that sequential importance sampling and particle filters both lead to proper weighting schemes ( corollary [ cor : proper - ssm ] ) .\\nthe unbiasedness property of particle filters observed in @xcite suggests a method of its own interest : a simple , general and parallelisable general particle ssm smoothing algorithm , with naturally available consistent confidence intervals ( proposition [ prop : particle - smooth - confidence ] ) .\\nwe are unaware of earlier works which explicitly state this potentially useful result , but we are mindful that it may be widely known by particle filter experts .\\nsection [ sec : asvar - clt ] discusses conditions under which our estimators admit a central limit theorem .\\nthe expressions for the asymptotic variance may be useful , for instance , when optimising the computational resources .\\nsection [ sec : block ] focuses on a so - called jump chain representation @xcite , where we propose block estimators that use the jump chain structure . in this context , it is possible to employ variance reduction techniques , such as stratification or rao - blackwellisation using the scheme of @xcite . section [ sec : delayed ] starts by making an explicit connection between the proper weighting schemes and the pseudo - marginal type mcmc , and then between the is type correction of mcmc and a two - stage delayed acceptance scheme @xcite .    in section [ sec : exp ] , we detail a more specific class of ssms with linear - gaussian state dynamics and explain a generic laplace approximation method that fits this class of models . we compare different algorithmic variations with a model having poisson observations and with a stochastic volatility model . we conclude in section [ sec : discussion ] with a discussion of the possible implications of our findings .\\nthroughout the paper , we consider general state spaces while using standard integral notation . if the model at hand is defined on euclidean space using standard probability densities , the following paragraph can be skipped .\\neach space @xmath0 is assumed to be equipped with a @xmath1-finite dominating measure ` @xmath2 \\' on a @xmath1-algebra denoted with a corresponding calligraphic letter , such as @xmath3 .\\nproduct spaces are equipped with the related product @xmath1-algebras and product dominating measures .\\nif @xmath0 is a subset of an euclidean space @xmath4 , @xmath2 is taken by default as the lebesgue measure and @xmath3 as the borel subsets of @xmath0 .\\nif @xmath5 is a probability density on @xmath0 , we denote the support of @xmath5 as @xmath6 , and the probability measure corresponding to @xmath5 with the same symbol @xmath7 .\\nif @xmath8 , we denote @xmath9 , whenever well - defined . for a probability density or measure @xmath5 on @xmath0 and @xmath10 ,\\nwe denote by @xmath11 the set of measurable @xmath8 with @xmath12 , and by @xmath13 the corresponding set of zero - mean functions .\\nif @xmath14 is a markov transition probability , we denote the probability measure @xmath15 , and the function @xmath16 .\\niterates of transition probabilities are defined recursively through @xmath17 for @xmath18 .\\nwe follow the convention @xmath19 , and if @xmath20 , we denote by @xmath21 the integers within the interval @xmath22 $ ] .\\nwe use this notation in indexing , so that @xmath23 , @xmath24 .\\nif @xmath25 , then @xmath26 or @xmath27 is void , so that for example @xmath28 means @xmath29 .\\nsimilarly , if @xmath30 is a vector , then @xmath31 and @xmath32 . we also use double - indexing , and write @xmath33 .\\nthe markov chain @xmath34 on @xmath0 is harris recurrent with invariant probability @xmath5 , if for every @xmath35 , and every initial distribution , the ergodic averages are strongly consistent , @xmath36 we call such a chain _ harris ergodic ( with respect to @xmath5)_.    recall that virtually all mcmc schemes are harris ergodic ( cf .\\n* ) , although in some cases careless implementation could lead to non - harris chains ( cf .\\nhereafter , @xmath37 is a probability density on @xmath0 and represents an approximation of a probability density @xmath38 of interest .\\n[ a : mcmc - is ] the density @xmath37 and the related markov chain @xmath39 satisfy    a.   @xmath34 is harris ergodic with respect to @xmath37 .\\nb.   [ item : support ] @xmath40 . c.   @xmath41 , where @xmath42 is a constant .\\nassumption [ a : mcmc - is ] is equivalent to a seemingly more general statement , where @xmath38 and @xmath37 are probability measures with @xmath43 . in particular , taking @xmath37 as the dominating measure on @xmath0 , then one can define @xmath44 , and the density of @xmath38 is the corresponding radon - nikodym derivative @xmath45 .\\nif assumption [ a : mcmc - is ] holds and it is possible to calculate the unnormalised importance weight @xmath46 pointwise , the chain @xmath39 can be weighted in order to approximate @xmath47 for every @xmath48 , using ( self - normalised ) importance sampling ( e.g. * ? ? ?\\n* ; * ? ? ?\\n* ) @xmath49{n\\\\to\\\\infty } \\\\frac{\\\\pi_a(w_u f)}{\\\\pi_a(w_u ) }      = \\\\pi(f ) ,      \\\\label{eq : mcmc - is}\\\\ ] ] as the harris ergodicity guarantees the almost sure convergence of both the numerator and the denominator .    in many cases of interest ,\\nthe importance weights @xmath46 can not be calculated or they are extremely costly to evaluate . instead , it is often possible to construct unbiased estimators of @xmath50 , which may be used in place of @xmath50 under mild conditions . in order to formalise such a setting ,\\nwe formulate the following abstract setting , which accommodates further generalisations of importance sampling type corrections .\\n[ a : aug ] suppose @xmath34 is a harris ergodic markov chain on @xmath0 with invariant probability @xmath37 , and @xmath51 are conditionally independent given @xmath34 taking values on a space @xmath52 , such that the distribution of @xmath53 depends only upon the value of @xmath54 : @xmath55    in what follows , we consider several such augmented chains .\\nnote that the mapping @xmath56 in is never needed in practice , but only introduced for theoretic purposes .\\ntechnically , @xmath56 is required to be a regular conditional distribution of @xmath53 given @xmath54 , but in practice , the concerns about the regularity of @xmath56 are usually unnecessary .\\n[ thm : aug - consistency ] suppose assumption [ a : aug ] holds .\\nthen , @xmath57 is a harris ergodic markov chain on @xmath58 with invariant probability @xmath59    theorem [ thm : aug - consistency ] follows from lemma [ lem : aug - prop ] in appendix [ app : aug ] .\\nlet us now consider the simple importance sampling type correction where @xmath50 are replaced with unbiased estimates @xmath60 .\\nthis idea has been suggested in more specific contexts earlier at least in @xcite .    [ a : unbiased ] suppose assumption [ a : mcmc - is ] holds , @xmath61 are independent random variables on @xmath62 conditional on @xmath34 , the distribution of @xmath60 depends only on the value of @xmath54 , and satisfies @xmath63 { \\\\overset{\\\\mathrm{a.s.}}{=}}w_u(x)$ ] for @xmath37-almost every @xmath64 .    [ prop : simple - is ] suppose assumption [ a : unbiased ] holds and denote @xmath65 the estimator @xmath66 is strongly consistent , @xmath67 almost surely , if either of the following hold :    a.   @xmath68 almost surely and @xmath48 .\\nb.   @xmath8 is such that @xmath69 , where @xmath70 $ ] .\\nthe results follow from theorem [ thm : aug - consistency ] , once we check that the functions @xmath71 and @xmath72 .    in proposition [ prop : simple -\\nis ] :    a.   the case @xmath68 coincides with the case where pseudo - marginal algorithms can be implemented , and provide similarly consistent estimators for all @xmath48 ; see proposition [ prop : proper - pseudo ]\\n. b.   when @xmath60 are allowed to take also negative values , we believe that @xmath66 might fail to be consistent for all @xmath48 , and an extra condition such as stated might be indeed necessary . c.   if the relative error in @xmath60 is bounded : there exists @xmath73 such that @xmath74 for all @xmath64 , then @xmath66 is guaranteed to be strongly consistent for all @xmath48 .\\nwe then focus on our main scenario , where both @xmath50 and @xmath75 may be estimated .\\nsuch a scenario occurs when @xmath37 is approximation of a marginal density , which we discuss in detail after we state the following abstract assumption .\\n[ a : super - general ] suppose assumption [ a : mcmc - is ] holds , let @xmath48 and let @xmath76 be @xmath77-valued random variables conditionally independent given @xmath34 , such that the distribution of @xmath53 depends only on the value of @xmath54 , and satisfies @xmath78 = f(x)w_u(x)\\\\qquad\\\\text{and}\\\\qquad      { \\\\mathbb{e}}[w_k\\\\mid x_k = x ] = w_u(x),\\\\ ] ] for @xmath37-almost every @xmath64 , and such that @xmath79\\\\ ] ] satisfies @xmath80 .\\n[ prop : super - general ] if assumption [ a : super - general ] holds , then @xmath81{n\\\\to\\\\infty } \\\\pi(f).\\\\ ] ]    follows from theorem [ thm : aug - consistency ] , because both @xmath82 and @xmath83 are in @xmath84 .    in the specific cases we consider later , we define @xmath85 , where @xmath86 are random variables which satisfy @xmath87 = c_w \\\\pi(x ) f(x ) ] \\\\qquad \\\\text{and}\\\\qquad { \\\\mathbb{e}}[u_k\\\\mid x_k = x ] = c_w \\\\pi(x).\\\\ ] ] in some cases , however , it may be that @xmath88 is unavailable , for instance if @xmath89 is a pseudo - marginal algorithm targeting @xmath37\\n. it may still be possible to estimate the ratio @xmath46 in an unbiased fashion .    in some scenarios\\n, it may be possible to use simply @xmath90 which are independent of @xmath60 ( or @xmath91 ) , with @xmath92=f(x)$ ]\\n. however , such @xmath90 may be more difficult to construct than dependent @xmath93 which satisfy assumption [ a : super - general ] .\\nit is clear that assumption [ a : unbiased ] and the assumptions in proposition [ prop : simple - is ] form a special case of assumption [ a : super - general ] , by taking @xmath94 .\\nthe following scheme formulates a case where @xmath34 targets @xmath88 , which is an approximation of a marginal density @xmath95 , and the aim is inference over a joint target density @xmath96 on an extended state space @xmath97 .\\nthe following assumption is similar to ( * ? ? ?\\n* definition 2.5.1 ) :    [ a : proper ] suppose assumption [ a : mcmc - is ] holds , @xmath98 and @xmath99 are conditionally independent given @xmath34 , and take values on @xmath100^m\\\\times{\\\\mathbb{r}}$ ] with @xmath101 for all @xmath102 , and such that for every @xmath103 , the random variables @xmath104 satisfy @xmath105 { \\\\overset{\\\\mathrm{a.s.}}{=}}w_u(x ) f^*(x)$ ] , where @xmath106 for @xmath37-almost every @xmath64 .\\n[ prop : proper - consistency ] suppose that assumption [ a : proper ] holds .\\nthen , the estimator @xmath107 satisfies @xmath108 almost surely , if either of the following hold :    a.   [ item : proper - positive ] @xmath68 almost surely for all @xmath109 and @xmath103 . b.   [ item : proper - general ] @xmath103 and @xmath110\\\\ ] ] satisfies @xmath80 .\\nthis is a special case of proposition [ prop : super - general ] , because assumption [ a : proper ] together with , which is also implied by , implies assumption [ a : super - general ] .\\nthe following immediate result records that importance sampling estimators fit assumption [ a : proper ] naturally .\\n[ prop : augmented - is ] suppose @xmath111 such that @xmath112 is a probability density for each @xmath64 and such that @xmath113 .\\nlet @xmath114 and denote @xmath115 where @xmath42 is some constant , and let @xmath116 .\\nthen , @xmath117 form a proper weighting scheme ( assumption [ a : proper ] ) , and therefore , for any @xmath103 , @xmath118{n\\\\to\\\\infty } \\\\pi^*(f).\\\\ ] ]    the next result shows that sub - sampling within a proper weighting scheme leads to a proper weighting scheme .\\n[ prop : randomise - proper ] suppose that @xmath117 forms a proper weighting scheme .\\nlet @xmath119 be random variables conditionally independent of @xmath120 such that @xmath121 .\\nthen , @xmath122 forms a proper weighting scheme .    proposition [ prop : randomise - proper ] follows by observing that @xmath123 is a proper weighting scheme , with @xmath124 .\\nproposition [ prop : randomise - proper ] implies that the estimator @xmath125{n\\\\to\\\\infty } \\\\pi^*(f)\\\\ ] ] under the conditions stated in proposition [ prop : proper - consistency ] .\\nsuch extra randomisation generally leads to a higher asymptotic variance , but may be preferable if the weighted sample @xmath126 needs to be stored for further analysis .    in case of non - negative weights ,\\nit is easy to see that the following convex combinations of proper weighting schemes remain proper .\\n[ prop : convex - proper ] suppose @xmath127 and @xmath128 forms a proper weighting scheme for each @xmath129 , @xmath130 are conditionally independent given @xmath34 in the sense of assumption [ a : aug ] , and @xmath131 almost surely for all @xmath132 .\\nthen , the random variables @xmath133 form a proper weighting scheme @xmath134 for every @xmath135^n$ ] with @xmath136 .\\nwe discuss the implication of proposition [ prop : augmented - is ] in case of state space models in section [ sec : ssm ] .\\nin such a setting , @xmath89 stands for the ( static ) parameters of the model , @xmath137 for the sampled latent state trajectories , and @xmath138 for the corresponding ( unnormalised ) importance weights .\\nwe recall also the unbiasedness property of particle filters which leads to proper weighting schemes .\\nstate space model ( ssm ) techniques are applied in a wide variety of applications ( cf . * ? ? ?\\nwe assume the existence of a family of ssms indexed by a static parameters @xmath139 taking values on a space @xmath140 .\\nthe model consists of a sequence of latent ( unobserved ) states @xmath141 taking values in @xmath142 , and observations @xmath143 taking values in @xmath144 .\\nmore specifically , the latent states @xmath141 form a markov chain with initial density @xmath145 and state transition densities @xmath146 for @xmath147 .\\nthe observations @xmath143 are conditionally independent given @xmath141 and follow the observation model @xmath148 .\\nthis model defines the following probability density on @xmath149 : @xmath150 where , by convention , @xmath151 .\\nthe inference is based on the so - called smoothing distribution @xmath152 where @xmath153 is a normalising constant .    in a bayesian state space model ,\\nthe parameters @xmath139 are regarded random as well , with a prior density @xmath154 , which leads to the following joint density on @xmath155 : @xmath156 the ( full ) bayesian inference is based on the posterior distribution @xmath157 with overall normalising constant @xmath158 .\\nlet us start with a description of a sequential importance sampling procedure , which requires user - supplied ` proposal \\' distributions @xmath159 for @xmath160 .\\n[ alg : sis ] input : @xmath143 and @xmath161 .\\na.   for @xmath160 , sample @xmath162 and calculate @xmath163    output : @xmath164 , where @xmath165 and @xmath166 .\\nif the following well - known support condition holds , we recall that algorithm [ alg : sis ] provides a weighting which is shown to lead to proper weighting in corollary [ cor : proper - ssm ] .\\n[ a : support ] suppose that @xmath167 for all @xmath160 , where @xmath168    [ prop : sis - proper ] suppose assumption [ a : support ] holds , then the random variables @xmath169 , @xmath170 and @xmath171 produced by algorithm [ alg : sis ] satisfy @xmath172      & = p^{(\\\\theta)}(y_{1:t } ) \\\\int p^{(\\\\theta)}(\\\\alpha_{1:t}\\\\mid y_{1:t } )      f(\\\\alpha_{1:t } ) { \\\\mathrm{d}}\\\\alpha_{1:t}.      \\\\label{eq : ssm - proper}\\\\end{aligned}\\\\ ] ] for any function @xmath173 with the above integral well - defined .\\nproposition [ prop : sis - proper ] follows from proposition [ prop : augmented - is ] , because @xmath174 are ratios of @xmath175 and @xmath176 , which is the law of the independent trajectories @xmath177 .\\nlet us consider next a particle filter algorithm @xcite ; see also the monographs @xcite , which is often much more efficient in practice .\\nthe algorithm is based on same user - supplied proposal distributions @xmath178 as sis , but requires also a ` resampling \\' distribution @xmath179 , which draws a sample @xmath180 from a discrete probability mass @xmath181 .\\nwhenever the index ` @xmath182 \\' appears in the algorithm , it takes values @xmath183 .\\n[ alg : pf ] input : @xmath143 and @xmath161 .\\na.   sample @xmath184 , set @xmath185 and calculate @xmath186    for @xmath147 , do    a.   sample @xmath187 , where @xmath188 b.   sample @xmath189 and set @xmath190 . c.   calculate @xmath191    if @xmath192 in algorithm [ alg : pf ] , the algorithm may be terminated immediately  all the estimators considered in proposition [ prop : particle - proper ] below equal to zero then .\\nwe prefer to avoid such explicit stopping mechanism for mathematical convenience .\\nwe require the following well - known unbiasedness condition on the resampling procedure which is satisfied by the common multinomial , stratified , residual and systematic resampling methods ( cf . * ? ? ?\\n[ a : resampling ] the resampling procedure satisfies @xmath193=m\\\\bar{\\\\omega}^{(j)}.\\\\ ] ] for any @xmath194 and any probability mass vector @xmath181 .\\n[ prop : particle - proper ] suppose that assumptions [ a : support ] and [ a : resampling ] hold , let @xmath161 and suppose @xmath173 is such that the integral in well - defined .\\nconsider the random variables generated in algorithm [ alg : pf ] , and let @xmath195 .\\na.   [ item : filter - smoother ] the random variables @xmath164 where @xmath196 and @xmath197 satisfy .\\ndefine for @xmath147 , and any @xmath198 , the backwards sampling probabilities @xmath199 and in case the sum is zero , set @xmath200 .\\na.   [ item : backwards - sampling ] let @xmath201 be a random variable generated recursively by @xmath202 , and @xmath203 for @xmath204 .\\nfor any @xmath103 , the random variables @xmath205 satisfy , where @xmath206 and @xmath207 . b.   [ item : fwd - bwd - smoothing ] if @xmath103 and @xmath208 for some @xmath209 , that is , @xmath210 is constant in all coordinates except @xmath211 and @xmath212 , then , the random variables @xmath213 satisfy ( with @xmath214 ) , where a.   @xmath215 , b.   @xmath216 , and where c.   [ item : smoothing - weights ] @xmath217 and @xmath218 for @xmath219 . c.   [ item : fwd - bwd - smoothing2 ] if @xmath103 and @xmath220 for some @xmath221 , then the random variables @xmath222 satisfy ( with @xmath214 ) , where @xmath223 and @xmath224 are defined in .    we provide a self - contained but concise proof of proposition [ prop : particle - proper ] in appendix\\n[ app : particle ] .\\nproposition [ prop : particle - proper ]    * corresponds to so - called filter - smoother @xcite .\\nthis property was shown in ( * ? ? ?\\n* theorem 7.4.2 ) , although only in case of multinomial resampling .\\nthe same property is behind the original version of the particle marginal metropolis - hastings @xcite ; see also remark [ rem : proper - mcmc ] .\\n+ the statement holds also when the particle filter is applied with a general sequence of distributions @xmath225 rather than the ssm @xcite .\\n* corresponds to backwards simulation , is analogous to the developments in conditional sequential monte carlo with backwards sampling @xcite .\\nthis allows us to draw any number of independent trajectories ( indices ) , and average them , in order to satisfy . * and its special case correspond to the forward - backward smoother @xcite\\n; see also @xcite .\\nit can be seen as rao - blackwellised version of , but is applicable only when @xmath210 is a specific form .\\nthis scheme can lead to lower variance , but it also has square complexity in @xmath226 , rendering it inapplicable with large @xmath226 .\\nwe next note how propositions [ prop : sis - proper ] and [ prop : particle - proper ] ensure that both algorithms [ alg : sis ] and [ alg : pf ] yield a proper weighting scheme .\\n[ cor : proper - ssm ] let @xmath227 be a markov chain which is harris ergodic with respect to @xmath37 , and let @xmath228 correspond to    a.   independent runs of algorithm [ alg : sis ] with parameters @xmath227 , respectively , or b.   independent runs of a algorithm [ alg : pf ] with parameters @xmath227 , and the random variables defined in proposition [ prop : particle - proper ] or    then , the random variables @xmath229 with @xmath230 provide a proper weighting scheme for target distribution @xmath231 ( assumption [ a : proper ] ) .\\nsimilarly , if @xmath228 correspond to    a.   [ item : pf - marginal - inference ] independent runs of a algorithm [ alg : pf ] with parameters @xmath227 , and the random variables defined in proposition [ prop : particle - proper ] or ,    then @xmath229 provide a proper weighting scheme for the marginal distribution @xmath232 or @xmath233 , respectively .    corollary [ cor : proper - ssm ] is stated for a single marginal ( pair ) , but it is clear that we may estimate simultaneously several marginal ( pairs ) , so that proposition [ prop : particle - proper ] applies for every function which is of the form @xmath234 and proposition [ prop : particle - proper ] whenever @xmath235 .\\nit is possible to consider also more general classes of functions ; see for instance the discussion in @xcite .\\nwe next discuss one implication of proposition [ prop : particle - proper ] outside the main focus of this paper .\\nnamely , in the general state space smoothing context , that is , where @xmath161 is constant , proposition [ prop : particle - proper ] yields easily parallelisable particle smoothing algorithms , which also admit easily calculated consistent confidence interval estimates .\\n[ prop : particle - smooth - confidence ] suppose @xmath161 is fixed , and let @xmath228 correspond to independent random variables as defined in proposition [ prop : particle - proper ] .\\na.   if the corresponding conditions of proposition [ prop : particle - proper ] are satisfied , then the estimator @xmath236{n\\\\to\\\\infty } \\\\mu_f { \\\\mathrel{\\\\mathop:}=}\\\\int      p^{(\\\\theta)}(\\\\alpha_{1:t}\\\\mid y_{1:t } ) f(\\\\alpha_{1:t}){\\\\mathrm{d}}\\\\alpha_{1:t},\\\\ ] ] where @xmath237 , whenever the integral is well - defined . b.   if also @xmath238\\\\big)^2\\\\big]<\\\\infty$ ] , then @xmath239 \\\\xrightarrow[\\\\mathrm{d}]{n\\\\to\\\\infty }      n(0,\\\\sigma^2 ) ,      \\\\qquad\\\\text{where}\\\\qquad      \\\\sigma^2 { \\\\mathrel{\\\\mathop:}=}\\\\frac{\\\\sigma_*^2}{p^{(\\\\theta)}(y_{1:t})^2}.\\\\ ] ] c.   if in addition @xmath240<\\\\infty$ ] , then @xmath241 , almost surely , where @xmath242\\\\bigg)^2.\\\\ ] ]    the result of proposition [ prop : particle - smooth - confidence ] is straightforward and probably widely known , but we highlight it here because of the practical consequences it has for parallel particle smoothing .\\na.   proposition [ prop : particle - smooth - confidence ] ensures that , under the given conditions , the confidence intervals @xmath243 $ ] are asymptotically consistent , where @xmath244 corresponds to the desirable standard gaussian quantile .\\nb.   calculation of consistent confidence intervals for one realisation of a particle smoothing algorithm is not a straightforward task in general .\\nanother recently suggested method @xcite relies on unbiased estimators obtained by coupling of conditional sequential monte carlo chains and randomisation as suggested by @xcite . c.\\nif the particle filter estimates satisfy a variance asymptotic such as @xmath245 where @xmath246 correspond to a particle filter run with @xmath226 particles , then if @xmath226 is taken sufficiently large , the estimate as in proposition [ prop : particle - smooth - confidence ] is nearly as efficient than the single particle filter estimate with @xmath247 particles , but benefits from easy parallel implementation .\\nd.   even estimates which have quadratic cost ( in @xmath226 ) , as in proposition [ prop : particle - proper ] and , may be of practical interest in some scenarios . then , choosing an optimal @xmath226 leads to a non - trivial optimisation task .\\nthe asymptotic variance is a common efficiency criterion for markov chains .\\nwhen a central limit theorem ( clt ) holds , the limiting variance coincides with the asymptotic variance .\\n[ def : asvar ] suppose the markov chain @xmath34 on @xmath0 has transition probability @xmath14 which is harris ergodic with respect to invariant probability @xmath37\\n. for @xmath248 , the asymptotic variance of @xmath210 with respect to @xmath14 is @xmath249\\\\bigg)^2,\\\\ ] ] whenever the limit exists in @xmath250 $ ] , where @xmath251 stands for the _ stationary markov chain _ with transition probability @xmath14 , that is , with @xmath252 .\\n[ def : clt ] the markov chain @xmath34 satisfies a clt for @xmath210 , if @xmath253 , and for any initial distribution , @xmath254      \\\\xrightarrow{n\\\\to\\\\infty } n\\\\big(0,{\\\\mathrm{var}}(f , p)\\\\big),\\\\ ] ] in distribution . in case @xmath255 , the limit is a unit mass at zero .    in order to guarantee a clt for a markov chain\\n, some form of rate of convergence is necessary .\\nwe follow jones @xcite and define the following :    [ def : convergence - rate ] suppose that a markov transition probability @xmath14 is harris ergodic with respect to a probability measure @xmath5 , and @xmath256 and @xmath257 are such that @xmath258 if @xmath259 , we say that @xmath14 admits @xmath260 convergence rate .\\ndifferent combinations of @xmath261 and @xmath262 imply a clt for different classes of functions ; see @xcite .\\nwe record the following summary of conditions which guarantee a clt for the augmented markov chain .\\nthe result follows easily from ( * ? ? ?\\n* theorem 9 ) , given the following simple result whose proof can be found in appendix [ app : clt ] .\\n[ lem : inherited - rate ] suppose that assumption [ a : aug ] holds , and the chain @xmath263 admits @xmath260 convergence rate .\\nthen , the chain @xmath57 admits a @xmath264 convergence rate , where @xmath265 .\\nhereafter , we denote by @xmath266 the transition probability of @xmath267 and recall that @xmath268 is given in    [ prop : aug - clt ] suppose that assumption [ a : aug ] holds , and @xmath34 satisfies definition [ def : convergence - rate ] with some @xmath260 . if @xmath269 for some @xmath270 and if one of the following hold :    a.   [ item : uniform ] @xmath271 and @xmath34 is uniformly ergodic , that is , @xmath272 , b.   [ item : geometric ] @xmath273 for some @xmath274 and @xmath34 is geometrically ergodic , that is , definition [ def : convergence - rate ] holds with @xmath275 for some constants @xmath276 , c.   [ item : geometric - rev ] @xmath271 and @xmath34 is geometrically ergodic and reversible , d.   [ item : polynomial ] @xmath273 for some @xmath274 and @xmath34 is polynomially ergodic , @xmath277 with @xmath278 , and @xmath279 ,    then @xmath57 satisfies a clt for @xmath210 , with a finite limiting asymptotic variance @xmath280 where @xmath281 and @xmath282 are the conditional mean and variance , respectively : @xmath283    proposition [ prop : aug - clt ] is a consequence of lemma [ lem : inherited - rate ] , ( * ? ? ? * theorem 9 ) , and lemma [ lem : aug - prop ] and proposition [ prop : aug - asvar ] in appendix [ app : aug ] , because also reversibility of @xmath34 is inherited by @xmath57 .    if @xmath284 , then @xmath285 for all @xmath274 .\\nit follows that assuming @xmath286 in is enough to guarantee a clt for @xmath287 .\\nnote that the @xmath288-geometric ergodicity can be shown to be inherited easily as well ( * ? ? ?\\n* lemma 45 ) .\\nlet us finally apply these results to the estimator introduced in proposition [ prop : proper - consistency ] .    [\\nprop : proper - clt ] suppose conditions of proposition [ prop : proper - consistency ] are satisfied , which guarantee the strong consistency of @xmath66 given in , and suppose @xmath34 satisfies one of the conditions in proposition [ prop : aug - clt ] with @xmath289 . denote @xmath290.\\\\ ] ] if @xmath291 , then @xmath292 \\\\xrightarrow{n\\\\to\\\\infty } n(0,\\\\sigma_f^2)\\\\ ] ] in distribution , where @xmath293 and where @xmath294 and @xmath295 .\\nproof of proposition [ prop : proper - clt ] is given in appendix [ app : clt ] .\\nthe latter term in the asymptotic variance expression of proposition [ prop : proper - clt ] contains the contribution of the importance sampling noise .\\nif the estimators @xmath296 are made increasingly accurate , in the sense that @xmath297 becomes negligible , the limiting case corresponds to an importance sampling corrected approximate mcmc , and evaluating the conditional expectation @xmath298 .\\nwe conclude this section by noticing that the contribution of the importance sampling correction in asymptotic variance as in proposition [ prop : proper - clt ] can be estimated in a straightforward manner .\\n[ prop : importance - var ] suppose conditions of proposition [ prop : proper - consistency ] are satisfied , which guarantee the strong consistency of @xmath66 given in .\\nsuppose also that @xmath299 where @xmath300 is defined in proposition [ prop : proper - clt ] , and @xmath301 , where @xmath302 $ ] .\\nthen , the estimator @xmath303 satisfies @xmath304 almost surely as @xmath305 .\\nproof of proposition [ prop : importance - var ] is given in appendix [ app : clt ] .\\nthe estimator @xmath306 in proposition [ prop : importance - var ] provides a _ lower bound _\\nestimate for the clt variance @xmath307 of @xmath66 .\\nit can provide useful information about the importance sampling noise contribution , and may be used as an optimisation criteria for the accuracy of the related estimators .\\nestimation of the full asymptotic variance @xmath308 , which includes the marginal chain contribution @xmath309 , is not a straightforward task in general ; see for example @xcite and references therein .\\nmany mcmc algorithms include an accept - reject mechanism , which results in blocks of repeating values @xmath310 . when @xmath50 or @xmath311 are estimated , an obvious approach is to construct a single estimator which is used for the whole block .\\nthis may allow for variance reduction , for instance when replacing simple independent importance sampling with block - wide stratified estimator , or using particle filter estimators as in proposition  [ prop : particle - proper ] , and choosing the number of particles proportional to the block length .\\nto formalise such an algorithm we consider the `` jump chain \\'\\' representation of @xmath34 ( cf .\\n[ def : jump ] suppose that @xmath34 is a markov chain with non - degenerate transition probability @xmath14 , that is , @xmath312 for all @xmath64 .\\nthe jump chain @xmath313 with corresponding durations @xmath314 of @xmath34 is defined as follows : @xmath315 and @xmath316 , and then recursively @xmath317 where @xmath318 .\\nwe note the following observations regarding definition [ def : jump ] :    a.   if @xmath14 is harris ergodic with respect to @xmath37 which is non - trivial ( that is @xmath319 for any @xmath64 ) , then necessarily @xmath312 for all @xmath64 , so definition [ def : jump ] is consistent . b.   if @xmath34 corresponds to a metropolis - hastings chain , with non - diagonal proposal distributions @xmath320 ( that is , @xmath321 for every @xmath64 ) , then the jump chain @xmath322 is simply the accepted states , and @xmath323 is the number of rejections occurred at state @xmath322 .\\nthe importance sampling type estimator which we consider is of the following general form :    [ a : block - super - general ] suppose that assumption [ a : mcmc - is ] holds , let @xmath313 denote the corresponding jump chain ( definition [ def : jump ] ) , and assume @xmath48 .\\nsuppose that @xmath324 are conditionally independent random variables on @xmath77 given @xmath313 , whose distribution depends only on the value of @xmath325 , and which satisfy @xmath326      = \\\\frac{w_u(x)f(x)}{\\\\alpha(x)}\\\\qquad\\\\text{and}\\\\qquad      { \\\\mathbb{e}}[\\\\tilde{w}_k\\\\mid \\\\tilde{x}_k = x ] = \\\\frac{w_u(x)}{\\\\alpha(x)},\\\\ ] ] for @xmath37-almost every @xmath64 , and @xmath327\\\\ ] ] satisfies @xmath328 .\\n[ prop : block - super - general ] suppose assumption [ a : block - super - general ] holds , then @xmath329{n\\\\to\\\\infty } \\\\pi(f).\\\\ ] ]    proposition [ prop : jump - properties ] implies that the jump chain @xmath322 corresponding @xmath89 is harris ergodic with invariant probability @xmath330 .\\nthe rest follows from proposition [ prop : super - general ] .\\nwe consider next the most obvious block estimator , based on @xmath314 , corresponding to the jump chain and assumption [ a : super - general ] .\\n[ a : block - natural - general ] suppose that assumption [ a : mcmc - is ] holds , and let @xmath331 denote the corresponding jump chain ( definition [ def : jump ] ) .\\nlet @xmath332 correspond to a proper weighting scheme ( assumption [ a : proper ] ) , such that the variables in the scheme may depend on both @xmath325 and @xmath333 , and such that @xmath334      = w_u(x)f^*(x),\\\\ ] ] for all @xmath103 and @xmath37-almost every @xmath64 .\\n[ prop : block - natural - general ] suppose assumption [ a : block - natural - general ] holds , and @xmath335      \\\\quad      \\\\text{satisfies }      \\\\quad      \\\\pi_a(b)<\\\\infty .\\n\\\\label{eq : block - consistency - required}\\\\ ] ] then , @xmath336{n\\\\to\\\\infty } \\\\pi^*(f ) .\\n\\\\label{eq : block - natural}\\\\ ] ]    this follows from proposition [ prop : block - super - general ] , because the holding times @xmath337 are conditionally independent on @xmath322 and geometric with parameter @xmath338 , with expectation @xmath339 = 1/\\\\alpha(x)$ ] .\\ntherefore , @xmath340 and @xmath341 satisfy assumption [ a : block - super - general ] .    regarding assumption [ a : block - natural - general ] :    a.   the estimators\\n@xmath342 and @xmath60 which satisfy assumption [ a : block - natural - general ] may often come from a proper weighting scheme as in assumption [ a : proper ] , but here the parameter @xmath226 is allowed to depend on @xmath343 .\\nb.   the condition on @xmath344 $ ] is not optimal .\\nfor instance , it is easy to find examples where the estimator is strongly consistent , even though the conditional expectation is unbounded in @xmath345 . in practice , however , the estimators are usually chosen either as independent of @xmath345 , or increasingly accurate in @xmath345 ; see remark [ rem : block - varying - or - not ] . c.   the estimator of proposition [ prop : augmented - is ] coincides with a block estimator where @xmath346 . however , the block estimator offers more flexibility . for instance\\n, @xmath347 could be stratified , leading to a smaller variance .\\nd.   even though we believe that the estimators of the form are often appropriate , we note that general form of the estimator as given in assumption [ a : block - super - general ] may be useful in some cases , in order to further reduce variance . for instance , assumption [ a : block - super - general ] accommodates the case where rao - blackwellised lower - variance estimators of @xmath348 are used instead of @xmath349 , as suggested in @xcite .\\nlet us then consider the asymptotic variance of the estimator .\\n[ thm : block - clt ] suppose assumption [ a : block - natural - general ] holds , consider the estimator @xmath66 in and assume holds , @xmath350 , where @xmath351 , and @xmath352 , where @xmath353.\\\\ ] ] if @xmath34 is @xmath288-geometrically ergodic , then @xmath354 \\\\xrightarrow{n\\\\to\\\\infty } n(0,\\\\sigma^2 )         \\\\qquad\\\\text{in distribution},\\\\ ] ] where the limiting variance can be given as : @xmath355 ,      \\\\label{eq : block - clt - var}\\\\ ] ] where @xmath356,\\\\ ] ] and @xmath14 stands for the markov transition probability of @xmath34 .\\nproof of theorem [ thm : block - clt ] is given in appendix [ app : jump ] .\\n[ rem : block - varying - or - not ] in theorem [ thm : block - clt ] asymptotic variance expression , the latter term corresponds to the contribution of noise in importance sampling estimates .\\na.   when @xmath60 and @xmath357 correspond to @xmath333 independent estimates and @xmath358 , we might have @xmath359 , which leads to @xmath360 , so the contribution of the is would be upper bounded by @xmath361 .\\nb.   similarly , if @xmath362 , which could occur when @xmath93 are independent of @xmath333 , then , @xmath363 and the contribution of is would be upper bounded by @xmath364 .\\nthe acceptance probability is lower bounded for geometrically ergodic chains @xcite , rendering the estimator a safe choice .    about the assumption of @xmath288-geometric ergodicity in theorem [ thm : block - clt ] :    a.   the asymptotic variance expression we derive for the ` numerator \\' part of our estimator holds whenever @xmath34 is reversible , due to a result of deligiannidis and lee @xcite .\\nthis does not , however , imply a clt .\\nb.   our proof relies on the existence of a solution @xmath365 to the poisson equation @xmath366 .\\nwe believe that the jump chain @xmath313 inherits a central limit theorem from the base chain @xmath34 under general conditions , similarly as in section [ sec : asvar - clt ] .\\nhowever , the jump chain @xmath313 does not inherit a convergence rate from @xmath34 in general , so a simple approach such as in section [ sec : asvar - clt ] does not apply .\\nwe start by making an explicit connection of proper weighting schemes with positive weights and pseudo - marginal type mcmc algorithms .\\n[ a : proper - pseudo ] suppose that @xmath367 is a probability density on @xmath97 , @xmath368 is a family of proposal densities on @xmath0 and @xmath369 is a family of probability distributions on @xmath370^m\\\\times[0,\\\\infty)$ ] such that for almost every @xmath64 and all @xmath103 , the variables @xmath371 satisfy @xmath372      = \\\\varpi(x ) \\\\int \\\\pi^*(z\\\\mid x )      f(x , z ) { \\\\mathrm{d}}z,\\\\ ] ] where @xmath373 .\\n[ alg : proper - pseudo ] suppose assumption [ a : proper - pseudo ] holds , let @xmath374 and @xmath375 and for @xmath102 iterate :    a.   draw @xmath376 and @xmath377 .\\nb.   with probability @xmath378 accept and set @xmath379 ; otherwise reject and set @xmath380 .\\n[ prop : proper - pseudo ] consider algorithm [ alg : proper - pseudo ] .\\nif @xmath381 is integrable and the markov chain @xmath57 is harris recurrent , then @xmath382{n\\\\to\\\\infty } \\\\pi^*(f)\\\\qquad\\\\text{for        every $ f\\\\in l^1(\\\\pi^*)$,}\\\\ ] ] if and only if assumption [ a : proper - pseudo ] holds with @xmath383 .    if @xmath381 is integrable\\n, then @xmath384 defines an unnormalised probability on @xmath385 .\\ndefine a composite proposal distribution @xmath386 , then it is straightforward to check that @xmath387 that is , algorithm [ alg : proper - pseudo ] is a metropolis - hastings with target @xmath388 , and therefore reversible with respect to @xmath389 .\\nif the chain is harris ergodic , then for every @xmath390 , the strong law of large numbers holds .\\n[ rem : proper - mcmc ] proposition [ prop : proper - pseudo ] connects proper weighting explicitly with the validity of pseudo - marginal type algorithms .\\nthis analogue also suggests that our is type correction may be applied instead of any pseudo - marginal method .\\nthe proper weighting formulation may also suggest variations of pseudo - marginal schemes .\\nfor example , proposition [ prop : convex - proper ] suggests about the possibility of using several independent particle filters in a particle marginal metropolis - hastings update , which may be of interest in parallel computing .\\nlet us then focus on delayed acceptance ( da ) mcmc @xcite , which is a multiple - stage acceptance method in metropolis - hastings algorithm , which is often based on an approximate target distributions in order to reduce computational effort of a metropolis - hastings algorithm .\\nwe focus in particular on the two - stage da scheme based on an approximation @xmath37 of the true probability density @xmath38 , which has obvious similarities with the importance sampling scheme suggested earlier .\\nsuch da scheme with proposal densities @xmath368 can be summarised as follows :    [ alg : da ] suppose @xmath391 and for @xmath102 ,    a.   [ item : proposal ] generate a proposal @xmath392 b.   [ item : first - stage ] with probability @xmath393 continue to step , otherwise reject and set @xmath394 . here , @xmath395 c.   [ item : second - stage ] with probability @xmath396 set @xmath397 , otherwise reject and set @xmath398 .\\nthe steps and alone would implement a regular metropolis - hastings step targeting @xmath37 , whereas the second - stage acceptance step alone would implement the accept - reject step of independence metropolis - hastings algorithm with proposal @xmath37 .\\nthe rationale behind the da scheme is based on the same two assumptions which make the importance sampling corrected mcmc appealing : the evaluation of @xmath37 costs less than that of @xmath38 , and @xmath37 approximates @xmath38 reasonably well .\\nthe former directly affects the expected cost of the algorithm : if @xmath399 is rejected at step , there is no need to calculate @xmath400 , implying lower expected cost .\\nthe latter assumption guarantees that the second - stage acceptance rate is typically close to one .\\neven though the da version is guaranteed to be worse than direct metropolis - hastings targeting @xmath38 in the peskun sense @xcite , the lower cost often outweighs the difference by allowing a greater number of samples to be drawn with da .\\nwe note that a proper weighting scheme ( assumption [ a : proper ] ) with non - negative weights leads to a valid da scheme .    [ a : da - proper ] suppose assumption [ a : proper - pseudo ] holds with @xmath401 and a @xmath37 is a probability density on @xmath0 with @xmath40 .\\n[ alg : da - proper ] suppose assumption [ a : da - proper ] holds , and @xmath402 are proposal densities as in algorithm [ alg : da ] .\\ndefine the markov chain @xmath267 with initial values @xmath403 and @xmath404 , and for @xmath102 as follows :    a.   generate a proposal @xmath392 .\\nb.   [ item : first - stage2 ] with probability @xmath393 continue to step , otherwise reject and set @xmath405 . c.   [ item : second - stage2 ]\\ngenerate @xmath406 , then with probability @xmath407 accept and set @xmath408 , otherwise reject and set @xmath380 .    [\\nprop : da - valid ] if assumption [ a : da - proper ] holds and @xmath402 are such that algorithm [ alg : da - proper ] defines a harris recurrent markov chain @xmath57 , then algorithm [ alg : da - proper ] satisfies @xmath409{n\\\\to\\\\infty } \\\\pi^*(f ) ,      \\\\qquad\\\\text{for every $ f\\\\in l^1(\\\\pi^*)$.}\\\\ ] ]    the proof of proposition [ prop : da - valid ] follows from @xcite , because algorithm [ alg : da - proper ] is a valid da version of algorithm [ alg : proper - pseudo ] .\\nproposition [ prop : da - valid ] highlights that when a da - mcmc scheme is of the pseudo - marginal form as algorithm [ alg : da - proper ] , the importance sampling type correction as we suggest is applicable , and a natural alternative for da - mcmc .\\nthe efficiency of da - mcmc was studied by banterle , grazian , lee and robert @xcite .\\nsherlock , thiery and golithly @xcite studied the efficiency of pseudo - marginal with da as in algorithm [ alg : da - proper ] , with diffusion limit techniques .\\ncontrary to the is scheme , it is much harder to study the effect of noisy importance weights with da in general , as the noise affects the markov chain convergence rate .\\nhowever , the authors @xcite are able to provide useful heuristic how to tune the accuracy of the pseudo - marginal da method .\\nwe now turn into an illustration of our generic framework with more specific state space models .\\nsection [ sec : lin - gauss - ssms ] discusses an important sub - class of state space models , where the state dynamics are linear - gaussian .\\nwe describe a general method which is based on a gaussian approximation of the conditional smoothing distribution .\\na special case of the general state space model introduced in section [ sec : ssm ] is a model where both @xmath142 and @xmath144 are euclidean , and the state dynamics @xmath410 are linear - gaussian , but the observational distribution @xmath411 may be non - gaussian and/or non - linear . more specifically , @xmath412 are defined through @xmath413 and @xmath414 our approach allows , for instance , exponential family observation models with gaussian , poisson , binomial , negative binomial , and gamma distributions , and a stochastic volatility model .\\nwe discuss the specific models later in sections [ poisson ] and [ sv - model ] .\\ncompared to the general state space form , this class of models may seem restrictive , but it still contains a large number of commonly used statistical models , such as ( generalised ) structural time series models , cubic splines , generalised linear models , and autoregressive integrated moving average ( arima ) models .\\nthe benefit of working within this subset of general state space models is that we have access to approximations taking advantage of the specific structure of the model .\\nour approximation scheme , based on @xcite , relies on a laplace approximation @xmath415 of the conditional densities @xmath416 .\\nthe approximating gaussian model is found by using an iterative process closely related to iterative reweighted least squares algorithm used in generalised linear models context @xcite . in the approximating gaussian model\\n, the observation equation is replaced by a linear - gaussian one @xmath417 where the pseudo - observations @xmath418 and their variances @xmath419 are based on the first and second derivatives of @xmath420 with respect to @xmath421 at @xmath422 , the conditional mode estimate of the approximate model @xcite .\\nour approximation of the marginal density @xmath37 is based on the marginal likelihood decomposition @xmath423,\\\\ ] ] where @xmath424 , the terms @xmath425 correspond to the product of pseudo - observation densities , and the expectation is taken with respect to @xmath426 .\\nif the approximation @xmath427 is close to @xmath428 in the high - probability regions of @xmath426 , the expectation term is close to one , so we take @xmath429 our approximation defines , in fact , a joint approximate distribution @xmath430 , and we use @xmath431 as a proposal distribution in our importance sampling correction .\\nwe use the laplace approximation @xmath432 described in section [ sec : lin - gauss - ssms ] as the importance distribution , and we use the simulation smoothing algorithm @xcite to simulate antithetic pairs of @xmath141 from @xmath432 .\\nwe implemented the following inference algorithms in our experiments :    amcmc : :    approximate mcmc targeting @xmath433 , and for each    accepted @xmath139 , sampling one realisation from    @xmath434 with the    simulation smoother .\\nmcmc : :    exact pseudo - marginal mcmc ( algorithm [ alg : proper - pseudo ] ) algorithm    using importance sampling estimates with @xmath226 samples .\\nda : :    delayed acceptance version of the above ( algorithm [ alg : da - proper ] ) .\\nis1 : :    simple importance sampling estimator given in with @xmath226    samples .\\nis2 : :    blocked estimator as in , with is estimators based on @xmath226    samples , independent of @xmath435 . is - pf : :    this is similar to is2 but uses the bootstrap particle filter @xcite    ( algorithm [ alg : pf ] ) with @xmath226 samples , and with stratified    sampling @xcite .\\nthe filter - smoother estimate ( proposition    [ prop : particle - proper ] ) was used .    in all the mcmc methods , the adaptive random walk metropolis algorithm of @xcite with gaussian proposal distribution was used . the target mean acceptance rate\\nwas set to 0.234 . in the delayed acceptance ,\\nthe first - stage acceptance rate was optimised , and in the pseudo - marginal , the total acceptance rate was optimised . in all cases ,\\nthe adaptive phase was terminated after the burn - in phase .\\nall the experiments were conducted using the ` bssm ` @xcite package in ` r ` @xcite , which will be made available .\\nwe also experimented with the following algorithmic modifications :    global approximation : :    in the standard version , a laplace approximation is constructed for    every new proposed value of @xmath139 .\\nwe experimented also    faster but less accurate alternative , where we consider only one    series of pseudo - observations constructed at the maximum likelihood    estimate of @xmath139 .\\nparallel computation : :    we tested our implementation with different number of cores .\\nthe    number of cores used in the experiment is indicated in parenthesis    such as is2 ( 8) .\\nsub - sampling for memory constraints : :    depending on the problem , we might be interested in either computing    some given summary statistics of @xmath141 , which can    be performed recursively , or constructing a weighted sample from    @xmath436 . in order to save in    storage , we used sub - sampling as in proposition    [ prop : randomise - proper ] for the latter .    our main aim is to compare the performance of the da - mcmc with our is type correction schemes , using the same approximate marginal target distribution in both .\\nnote that , if parallel aspects are not taken into account , the is2 estimator has typically lower cost than da with same parameter @xmath226 . indeed ,\\nif the second - stage proposal would always be accepted , then da would require approximately the same number of computations as is2\\n. on the other hand , is1 has typically higher cost than da . because da usually has non - zero second - stage acceptance probability , da with @xmath226 samples has roughly the same cost as is1 using @xmath437 samples , where @xmath438 accounts for rejected second - stage proposals . because the noise in the weighting scheme affects the mixing of the mcmc and da , whereas is1 and is2 are based solely on the approximate marginal mcmc chain plus is type correction , the efficiency of the estimators is likely to be problem dependent .\\nnote also that mcmc and da methods require is steps also during the burn - in , whereas is1 , is2 and is - pf do not .\\nwe consider first the following special case of the model discussed in section [ sec : lin - gauss - ssms ] : @xmath439 with @xmath440 . for testing our algorithms\\n, we simulated @xmath441 observations from this model with @xmath442 fixed to @xmath443 , and @xmath444 fixed to @xmath445 .    based on our pilot experiments , the pseudo - marginal type mcmc performed well with this model using global marginal likelihood approximation , so we employed it for the experiments .\\nwe also observed that the number of importance samples @xmath226 could be set relatively low without affecting the mixing or accuracy much .\\nwe ran amcmc , mcmc , da , is1 , and is2 algorithms with @xmath446 , using 20,000 mcmc iterations with the first half discarded as burn - in .\\nwe used a uniform prior @xmath447 for the standard deviation parameters , where the cut - off parameter @xmath448 were set to @xmath449 based on the sample standard deviation of @xmath450 , where zeros were replaced with 0.1 .\\nresults were not sensitive to this upper bound .\\nwe performed two experiments . in the first\\n, we stored only one realisation of @xmath141 at each iteration , and the resulting samples was used to calculate the posterior mean and variance estimates of @xmath139 , @xmath451 , and @xmath452 . in the second experiment , we used all realisations from the simulation smoother and computed the posterior mean and variance estimates of @xmath451 and @xmath452 recursively .\\ntables [ table : poisson ] and [ table : poisson_summary ] summarise the results over 1000 repeated experiments as discussed above . in table\\n[ table : poisson ] , the approximate method gave slightly biased estimates for all variables , but with much less cost than exact mcmc .\\ndelayed acceptance decreased the computation costs by about two thirds , while the is2 ( with same @xmath226 and no parallelisation ) almost halves the costs of da . parallel computing with eight cores further decreased the computation time of is - corrected methods , but because of small @xmath226 , the speed - up was limited , as it is impossible to get much lower than with amcmc . using @xmath226 proportional to the block size ( is1 ) does not seem to bring significant benefits in this experiment .\\noverall , the main differences in da and is corrected methods are related to computational time .\\nthe parameter estimates and their standard errors are comparable .\\nthe pseudo - marginal mcmc provided perhaps slightly smaller standard errors , but due to the much higher computational time , the da and is2 appear preferable in this scenario\\n.    .means and the corresponding standard errors from 1000 mcmc runs for poisson model with 10 runs of simulation smoother per iteration , with one stored sample path . [ cols=\">,>,>,>,>,>,>\",options=\"header \" , ]      as an illustration of the estimators in proposition [ prop : particle - smooth - confidence ] , we simulated one realisation @xmath441 from a linear - gaussian ssm of the form @xmath453 with @xmath454 and @xmath455 . given @xmath441 , we estimated @xmath139 via maximum likelihood , resulting @xmath456 and calculated the exact smoothing means @xmath457 $ ] for @xmath458 .\\nwe performed particle smoothing using the filter - smoother ( fs ) and forward - backward smoother ( fbs ) ( proposition [ prop : particle - proper ] and , respectively ) , and repeated these computations @xmath459 times , from which we obtained 95% confidence intervals as in proposition [ prop : particle - smooth - confidence ] .\\nfigure [ fig : ci_plot ] shows confidence intervals from both methods with varying number of particles . for fbs , we used @xmath460 , and for fs , the corresponding squares @xmath461 , making the computational times roughly comparable .\\nwe note that in our implementation , fbs was about 1.4 times slower than fs on average .\\nit appears that in this experiment , fs outperformed fbs with comparable computational cost .\\n$ ] with @xmath462 , using forward - backward smoother algorithm ( fbs ) , and filter - smoother algorithm ( fs ) with varying number of particles ( note that algorithms use different amount of particles ) .\\nred line corresponds to the exact value computed by kalman smoothing . ]    because fbs may be of interest in some other scenarios , we attempted to optimise the choice of @xmath226 for the fbs .\\nwe ran the fbs with varying choices of @xmath226 as above , and using @xmath463 , respectively .\\nfigure [ fig : fbs_time ] shows the square root of average variance estimates @xmath306 of proposition [ prop : particle - smooth - confidence ] over time points @xmath160 , multiplied by the square root of computation time in minutes for different @xmath226 . in this experiment , @xmath464 or @xmath465 appeared good choices , but more importantly , the experiment suggests that the choice of @xmath226 in fbs may be a non - trivial optimisation problem .    .\\nwe suggested to use mcmc targeting a marginal of the posterior distribution and consequent importance sampling type estimation in order to perform easily parallelisable consistent inference over the full posterior distribution .\\nwe focused on state space models ( ssms ) , where importance sampling and particle filters provide natural and efficient correction mechanisms .\\nour specific application was based on a laplace approximation of ssms with linear - gaussian state dynamics , but arbitrary observation distributions .\\nour experiments are promising , and we believe that our methodology has potential to make exact inference feasible with a large class of models where only approximate inference is currently possible .\\nalternative approximation methods in the state space context include simple linearisation - based methods such as the extended kalman filter , moment matching , or variational approximations ; see the recent monograph @xcite .\\ngaussian latent variable models beyond the state space context could be handled with the laplace approximations , and more refined approaches such as the integrated nested laplace approximation @xcite could provide a useful approximate marginal .\\nwe note that , unlike with purely approximate inference , the approximate posterior @xmath37 does not need to be very accurate .\\nwe suggested importance ( is ) sampling type correction with sequential is and sequential monte carlo ( smc ) .\\nwe note that various smc variations , such as rao - blackwellised smc , and alternative resampling strategies , apply directly @xcite .\\nrandomised sequential quasi - monte carlo @xcite can be useful , when applicable .\\nsmc samplers can be useful beyond the state space context @xcite .\\nthe correction could be based on exact sampling for diffusions @xcite , and debiasing tricks and multilevel monte carlo @xcite can be useful for instance when the true target distribution is based on a continuum model , and only discretisations are accessible .\\nimportance sampling is known to be generally hard to implement efficiently in higher dimensions , and our setting is no exception .\\nthe fact that the approximation @xmath37 is required only on a marginal distribution may help , if the model admits a natural smaller dimensional parameter space , as our state space application .\\nindeed , in the state space context our approach relies on similar assumptions which make particle mcmc @xcite appealing , with the added assumption of the existence of suitable marginal approximation @xmath37 .\\nif such approximation is available , we suggest to take advantage of it , as an alternative to particle mcmc .    one potential issue of importance sampling correction , in general , is when the importance weight @xmath46 is unbounded . in our experiments\\n, we did not observe large values of importance weights , but this is likely due to the fact that our space @xmath466 was bounded . in case @xmath0\\nis unbounded , it may be that the ratio @xmath467 is unbounded , even if @xmath88 is a reasonable approximation in the ` centre \\' of @xmath0 . because bounded @xmath468 is generally desirable ,\\nwe suggest tempering as a possible remedy .\\nthat is , if @xmath37 is our original approximate marginal , the chain @xmath34 may be designed to target @xmath469 instead , where @xmath470 is a tempering parameter .\\nit may be of practical interest to construct the marginal chain @xmath34 using adaptive mcmc techniques such as @xcite or as discussed in the review @xcite .\\nwe note that our theory does not apply directly with adaptive mcmc , unless the adaptation is stopped after suitable burn - in , like we do in our experiments .\\nan obvious alternative to our approach is that , instead of running one markov chain targeting @xmath37 and then calculating conditional importance sampling type estimates , one may consider running multiple independent pseudo - marginal type mcmc methods in parallel .\\nwe note that while valid , this approach may suffer from bias if the mcmc chains are not run long enough , and burn - in ( and possible adaptation ) of each chain could be seen wasteful .\\nit is well - known that pseudo - marginal methods may be slow mixing when the importance weights have large variation  in the extreme case they may lose desirable rates of convergence such as geometric ergodicity @xcite .\\nlarge variation of importance weights affects also our method , but we believe that mcmc targeting the approximate target @xmath37 may be often easier to design in an efficient manner .\\nthe authors were supported by an academy of finland research fellowship ( grants 274740 and 284513 ) .\\nwe thank christophe andrieu for insightful remarks .\\nthroughout this section , suppose that @xmath471 is a markov transition probability on @xmath0 and @xmath472 is a markov transition probability from @xmath0 to a space @xmath52 .\\nwe consider here properties of an augmented markov transition probability @xmath473 defined on @xmath58 as follows : @xmath474 we first state the following basic result .\\n[ lem : aug - prop ] the properties of @xmath471 and the augmented chain @xmath473 are related as follows :    a.   [ lem : aug - mim - irr ] let @xmath475 denote the set of @xmath476-irreducibility measures of a markov transition probability @xmath471 , then @xmath477 b.   [ lem : aug - mim - mim ] the implications in hold when @xmath475 and @xmath478 are replaced with sets of maximal irreducibility measures of @xmath471 and @xmath473 , respectively . c.   [ item : aug - invar ] the invariant probabilities of @xmath471 and @xmath473 satisfy : @xmath479 the above implications hold also with invariance replaced by reversibility .\\nd.   [ item : aug - harris ] @xmath471 is harris recurrent if and only if @xmath473 is harris recurrent .\\ne.   [ item : aug - iterates ] suppose @xmath480 is measurable and such that @xmath481 and @xmath482 are well - defined\\n. then , for any @xmath483 , @xmath484    the inheritance of irreducibility measures ( [ lem : aug - mim - irr ] ) , maximal irreducibility measures ( [ lem : aug - mim - mim ] ) , invariant measures , and reversibility are straightforward .    for harris recurrence ,\\nlet the probability @xmath485 be a maximal irreducibility measure for @xmath471 , then @xmath486 is the maximal irreducibility measure of @xmath473 .\\nlet @xmath487 with @xmath488 , and choose @xmath489 such that @xmath490 , where @xmath491 with @xmath492 .\\nnotice that @xmath493 where @xmath494 are the hitting times of @xmath89 to @xmath495 .\\nthis concludes the proof because @xmath496 are independent bernoulli random variables with success probability at least @xmath497 .\\nthe converse statement is similar .    for\\n, it is enough to notice that for any @xmath498 and @xmath483 , it holds that @xmath499 .\\nwe next state the following generic result which gives an expression for the asymptotic variance of an augmented markov chain .\\n[ prop : aug - asvar ] let @xmath500 and define the conditional mean @xmath501 and the conditional variance @xmath502 , then @xmath503 whenever @xmath504 is well - defined .\\nlet @xmath267 be the stationary markov chain with transition probability @xmath473 .\\n@xmath505\\\\bigg ) \\\\\\\\      & = \\\\check{\\\\mu}(h^2 ) + \\\\frac{2}{n } \\\\sum_{i=1}^{n-1 }      \\\\sum_{\\\\ell=1}^{n - i } { \\\\mathbb{e}}[h(x_0,s_0)h(x_\\\\ell , s_\\\\ell)],\\\\end{aligned}\\\\ ] ] by stationarity .\\nwe may write by lemma [ lem : aug - prop ] , @xmath506      & = \\\\int \\\\check{\\\\mu}({\\\\mathrm{d}}x\\\\times { \\\\mathrm{d}}s ) h(x , s ) ( k^\\\\ell m_h)(x ) \\\\\\\\      & = { \\\\mathbb{e}}[m_h(x_0 ) m_h(x_\\\\ell)].\\\\end{aligned}\\\\ ] ] we deduce that @xmath507 where @xmath508 .\\nthe claim follows by taking limit @xmath305 .\\nwe finally state a simple result about the relationship of the solutions of the poisson equation for augmented chains , used in appendix [ app : jump ] .\\n[ lem : aug - poisson ] suppose that @xmath500 .\\nif there exists @xmath509 such that @xmath510 , where @xmath511 , then @xmath512    it is clear that @xmath513 and @xmath514 , so @xmath515\\nwe follow @xcite and prove the statement assuming that the resampling scheme @xmath516 satisfies the following property : @xmath517 for all @xmath518 .\\nwe can do this without loss of generality , because this condition would be satisfied if we applied an independent random permutation of indices to the resampling scheme satisfying assumption [ a : resampling ] .\\nthe statement is valid without such randomisation , because both the algorithm and the statements are independent of ordering .    for , define the filtrations @xmath519 and @xmath520 , and functions @xmath521 , and for @xmath522 @xmath523 clearly @xmath524 , and all @xmath525 are ( almost everywhere ) well - defined if the latter integral is well - defined .\\nlet us first observe that for @xmath147 , @xmath526      & =      { \\\\mathbb{e}}\\\\big [ { \\\\mathbb{e}}\\\\big [      \\\\omega_t^{(i ) }      f_t(\\\\bar{\\\\alpha}_{t-1}^{(a_{t-1}^{(i)})},\\\\alpha_t^{(i ) } )      { \\\\mathrel{\\\\big|}}{\\\\mathcal{g}}_{t-1 }      \\\\big ]      { \\\\mathrel{\\\\big|}}{\\\\mathcal{f}}_{t-1 }      \\\\big ]       \\\\label{eq : unbiased - step }       \\\\\\\\      & =      { \\\\mathbb{e}}\\\\big [      f_{t-1}(\\\\bar{\\\\alpha}_{t-1}^{(a_{t-1}^{(i ) } ) } )      { \\\\mathrel{\\\\big|}}{\\\\mathcal{f}}_{t-1 }      \\\\big ]       \\\\notag      \\\\\\\\      & =      \\\\sum_{i=1}^m \\\\bar{\\\\omega}_{t-1}^{(i ) }      f_{t-1}(\\\\bar{\\\\alpha}_{t-1}^{(i ) } ) .\\n\\\\notag\\\\end{aligned}\\\\ ] ] by applying recursively , we obtain @xmath527      & = { \\\\mathbb{e}}\\\\bigg[\\\\bigg(\\\\prod_{t=1}^{t-1 } \\\\frac{1}{m}\\\\omega_t^ * \\\\bigg )       \\\\frac{1}{m }      \\\\sum_{i=1}^m       { \\\\mathbb{e}}\\\\big [ \\\\omega_t^ *   \\\\bar{\\\\omega}_t^{(i ) }      f_t(\\\\bar{\\\\alpha}_{t-1}^{(a_{t-1}^{(i)})},\\\\alpha_t^{(i)}){\\\\mathrel{\\\\big|}}{\\\\mathcal{f}}_{t-1}\\\\big]\\\\bigg ] \\\\\\\\ & = { \\\\mathbb{e}}\\\\bigg[\\\\bigg(\\\\prod_{t=1}^{t-1 } \\\\frac{1}{m } \\\\omega_t^*\\\\bigg ) \\\\sum_{i=1}^m       \\\\bar{\\\\omega}_{t-1}^{(i ) } f_{t-1}(\\\\bar{\\\\alpha}_{t-1}^{(i ) } )     \\\\bigg ] \\\\\\\\     & = \\\\frac{1}{m}\\\\sum_{i=1}^m          { \\\\mathbb{e}}\\\\big [ \\\\omega_1^ * \\\\bar{\\\\omega}_t^{(i ) }      f_1(\\\\alpha_1^{(i)})\\\\big],\\\\end{aligned}\\\\ ] ] which equals to @xmath528 by a similar calculation as in .\\nthe statement is equivalent with @xmath529 = \\\\int p^{(\\\\theta)}(\\\\alpha_{1:t},y_{1:t } ) f(\\\\alpha_{1:t } )      { \\\\mathrm{d}}\\\\alpha_{1:t}.      \\\\label{eq : all - trajectories - proper}\\\\ ] ] similarly as above , we have for @xmath522 , @xmath530 \\\\\\\\      & = \\\\int \\\\sum_{j=1}^n \\\\bar{\\\\omega}_{t-1}^{(j ) } \\\\mu_t(\\\\alpha_t\\\\mid      \\\\alpha_{t-1}^{(j ) } ) g_t(y_t\\\\mid \\\\alpha_t )      \\\\frac {        \\\\bar{\\\\omega}_{t-1}^{(i_{t-1 } ) }        \\\\mu_t(\\\\alpha_t\\\\mid \\\\alpha_{t-1}^{(i_{t-1 } ) } )         } {    \\\\sum_{\\\\ell=1}^m       \\\\bar{\\\\omega}_{t-1}^{(\\\\ell ) }       \\\\mu_t(\\\\alpha_t\\\\mid\\\\alpha_{t-1}^{(\\\\ell ) } ) }   f_t(\\\\alpha_{1:t}^{(i_{1:t } ) } ) { \\\\mathrm{d}}\\\\alpha_t \\\\\\\\ & = \\\\bar{\\\\omega}_{t-1}^{(i_{t-1 } ) } f_{t-1}(\\\\alpha_{1:t-1}^{(i_{1:t-1})}),\\\\end{aligned}\\\\ ] ] and application of this recursively leads to .\\nthe result follows also from , by first summing over the indices @xmath531 .\\nthe last result follows as a special case of .\\nconsider the artificial trivial markov chain @xmath89 with @xmath532 for @xmath102 which is harris ergodic with respect to @xmath533 , with @xmath534 , and let @xmath535 .\\nthen , @xmath536 and @xmath537 with @xmath538 forms a proper weighting scheme for @xmath539 .\\nthe strong consistency follows from proposition [ prop : proper - consistency ] and the central limit theorem from proposition [ prop : proper - clt ] , because @xmath34 is trivially uniformly ergodic and @xmath540 .\\nnote that @xmath541 implying @xmath542 .\\nsimilarly , the convergence of @xmath543 follows from proposition [ prop : importance - var ] .\\nlet @xmath544 $ ] , then @xmath545 where @xmath546 $ ] .\\nthis implies that @xmath547    whenever @xmath548 , we may write @xmath354 = \\\\frac{n^{-1/2 }        \\\\sum_{k=1}^n w_k \\\\sum_{i=1}^n v_k^{(i ) }        \\\\bar{f}(x_k , z_k^{(i)})}{n^{-1 }        \\\\sum_{j=1}^n w_j}.      \\\\label{eq : clt - form}\\\\ ] ] the denominator converges to @xmath42 almost surely , so by slutsky s lemma it is enough to show that the numerator converges in distribution to @xmath549 .\\nthis follows from proposition [ prop : aug - clt ] , because @xmath291 .    for @xmath345 large enough such that @xmath550 for @xmath551 ,\\nwe may write @xmath552 the denominator converges to @xmath553 , and the numerator can be written as @xmath554,\\\\ ] ] and @xmath555 . the term @xmath556 , and because @xmath557 , the remainder terms @xmath558 and @xmath559 , as @xmath560 .\\nthe following proposition complements ( * ? ? ? * lemma 1 ) and @xcite , which are stated for more specific cases .\\n[ prop : jump - properties ] suppose @xmath89 is a markov chain with non - degenerate transition probability @xmath471 and @xmath322 its jump chain with corresponding holding times @xmath561 ( definition [ def : jump ] ) .\\nthen , the following hold :    a.   [ item : jump - trans ] @xmath322 is a markov chain with transition probability @xmath562 b.   [ item : jump - joint - trans ] the holding times @xmath561 are conditionally independent given @xmath322 , and each @xmath333 has geometric distribution with parameter @xmath563 . c.   [ item : jump - invar ] if @xmath471 admits invariant probability @xmath564 , then @xmath565 admits the invariant probability @xmath566 in addition , if @xmath471 is reversible with respect to @xmath5 , then @xmath565 is reversible with respect to @xmath567 . d.   [ item : psi - irreducibility ] @xmath89 is @xmath568-irreducible if and only if @xmath322 is @xmath568-irreducible , with the same maximal irreducibility measure .\\ne.   [ item : harris - recurrence ] @xmath89 is harris recurrent if and only if @xmath322 is harris recurrent .\\nthe expression of the transition probability is due to straightforward conditioning , and was observed in @xcite .\\nthe invariance follows from @xmath569 \\\\\\\\                & = \\\\frac{1}{\\\\mu(a)}\\\\bigg [ \\\\mu(a ) -\\n\\\\int_a \\\\mu({\\\\mathrm{d}}x)\\\\big(1-a(x)\\\\big)\\\\bigg ]                = \\\\tilde{\\\\mu}(a ) ,            \\\\end{aligned}\\\\ ] ] and the reversibility is shown in @xcite .\\nfor it is sufficient to observe that @xmath570 which holds because the sets @xmath571 and @xmath572 coincide .\\nsimilarly , holds because @xmath573 where @xmath574 and @xmath575 .          from the expression of the jump chain transition probability in proposition [ prop : jump - properties ]\\n, we obtain for any @xmath582 with @xmath583 well - defined , @xmath584 consequently , we observe that @xmath585 implying . because @xmath579 , lemma [ lem : poisson - variance ] and a straightforward calculation yield @xmath586 ,            \\\\end{aligned}\\\\ ] ] where @xmath587 .\\nsimilarly , by lemma [ lem : poisson - variance ] , @xmath588 which allows us to conclude .\\n[ lem : poisson - variance ] suppose the markov transition probability @xmath471 on @xmath0 is harris ergodic with respect to @xmath5 and @xmath589 . if there exists @xmath509 which solves the poisson equation , @xmath590 then @xmath591 and the markov chain @xmath34 following @xmath471 satisfies a clt .\\nlet @xmath14 , @xmath592 and @xmath266 be the markov transition probabilities of @xmath34 , @xmath313 and @xmath593 , respectively , and let @xmath37 , @xmath594 and @xmath268 be the corresponding invariant probabilities .\\nbecause @xmath34 and is @xmath288-geometric and @xmath595 , there exists a solution of poisson s equation @xmath596 such that @xmath597 .\\ngeometric ergodicity of @xmath34 implies also @xmath598 for @xmath37-almost every @xmath64 @xcite , so @xmath599 .\\nproposition [ prop : jump - variance ] implies that @xmath600 , and @xmath601 .\\ndenote then @xmath602 which has conditional mean @xmath603 , and @xmath604 is finite because @xmath605 \\\\le \\\\frac{2-\\\\alpha(x)}{\\\\alpha^2(x ) }      m_\\\\xi^{(2)}(x).\\\\ ] ] lemma [ lem : aug - poisson ] therefore implies that there exists @xmath606 such that @xmath607 , and consequently lemma [ lem : poisson - variance ] implies that @xmath593 satisfies a clt for @xmath608 .    for the expression of the asymptotic variance , proposition [ prop : aug - asvar ] applied with @xmath609 , @xmath610 , @xmath611 and @xmath612\\n, implies @xmath613 where , by the variance decomposition formula , @xmath614{\\\\mathrel{\\\\big|}}\\\\tilde{x}_k = x\\\\big ) \\\\\\\\      &\\n= v_{n\\\\xi}(x ) + w_u^2(x ) \\\\bar{f}^{*2}(x )      \\\\big(1-\\\\alpha(x)\\\\big)/\\\\alpha^2(x).\\\\end{aligned}\\\\ ] ] proposition [ prop : jump - variance ] implies that @xmath615.\\\\ ] ] if we write the scaled estimator analogously as in , we notice that the clt holds for the numerator , with asymptotic variance @xmath616/\\\\pi_a(\\\\alpha).\\\\ ] ] the denominator converges to @xmath617 , so we conclude with slutsky s lemma .\\na.  beskos , o.  papaspiliopoulos , g.  o. roberts , and p.  fearnhead .\\nexact and computationally efficient likelihood - based estimation for discretely observed diffusion processes .\\n, 68(3):333382 , 2006 .\\n( with discussion ) .'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['test'][10]['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4efdd108-000f-43d6-b2e1-9623aec0729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25591 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article: \n",
      "markov chain monte carlo ( mcmc ) has become a standard tool in bayesian analysis .\n",
      "the greatest benefit of mcmc is its generality  it is guaranteed to be consistent with virtually no assumptions on the underlying model . however , the practical applicability of mcmc generally depends on the dimension of the unknown variables , the number of data , and the computational resources available .\n",
      "furthermore , because mcmc is sequential in nature , it can be difficult to implement efficiently with modern parallel and distributed computing architectures ; see @xcite for general discussion about mcmc in challenging scenarios , and with parallel computing architectures .\n",
      "in recent years , a number of generic approximation methods have been developed for complex bayesian inference , for instance variational bayes ( cf .\n",
      "reviews * ? ? ?\n",
      "* ; * ? ? ?\n",
      "* ) , expectation propagation @xcite and laplace approximations @xcite .\n",
      "these methods have been reported to provide accurate enough inference for practical purposes in many scenarios , with a fraction of computing time compared with ( asymptotically ) exact approaches such as mcmc .\n",
      "one problem with approximate methods is that the related bias is typically hard or impossible to quantify , and in practice the only way to ensure the validity of the method is to compare the outcome with an exact method , such as mcmc .\n",
      "we promote , in broad terms , a well - known principle : the combination of an approximate method for ` preliminary ' inference , and importance sampling type correction of the output .\n",
      "our primary focus is on the approximation of a marginal distribution ( or marginal likelihood ) and the use of mcmc for approximate marginal inference .\n",
      "the outcome of the mcmc is corrected with importance sampling type estimates .\n",
      "we detail minimal requirements which lead to strongly consistent estimators and give general conditions under which central limit theorems hold .\n",
      "our setting highlights explicitly the connection of importance sampling type correction and recently developed pseudo - marginal type mcmc @xcite , such as particle mcmc @xcite , grouped independence metropolis - hastings ( gimh ) @xcite , approximate bayesian computation mcmc @xcite , the algorithm for estimation of discretely observed diffusions suggested in @xcite , and using annealed importance sampling @xcite .\n",
      "it also enables the use of debiased estimators as suggested in @xcite .\n",
      "all these methods are often computationally expensive , but our approach is based on independent realisations of such importance sampling type estimates , which can be calculated efficiently in parallel .\n",
      "we focus on bayesian state space models , where importance sampling and particle filters are used for correction .\n",
      "we believe that the setup is useful much more generally , for instance in latent variable models or discretisations of continuous models , such as diffusions or inverse problems .\n",
      "importance sampling correction of mcmc has been suggested early in the mcmc literature ; see at least @xcite .\n",
      "it has been used , for instance , when estimating bayes factors using a single mcmc output @xcite .\n",
      "bhattacharya @xcite has suggested a consistent estimator based on regeneration and tan , doss and hobert @xcite give such estimators in case of using multiple markov chains .    using unbiased estimators of importance weights in this context\n",
      "has been suggested at least in @xcite , who consider a generalisation of the pseudo - marginal method , allowing for likelihood estimators that occasionally take negative values .\n",
      "this allows , for instance , using the recently discovered ` debiasing tricks ' @xcite in order to construct an unbiased estimator .\n",
      "similar tricks , which would lead into non - negative unbiased estimators , as required by the pseudo - marginal method , may be unavailable ( cf . * ? ? ?\n",
      "quiroz , villani and kohn @xcite applied the is correction in the data sub - sampling context .    nested sampling has also appeared in many forms in the monte carlo literature .\n",
      "the smc^2^ algorithm @xcite is based on an application of nested sequential monte carlo steps , which has similarities with our framework , and the is^2^ method @xcite focuses on the case where the preliminary inference is based on importance sampling .\n",
      "we focus on the mcmc approximation of the marginal distribution , which we believe often to be easily implementable in practice .\n",
      "the markov chain nature of the marginal monte carlo approximation comes also with some extra theoretical issues which we address .\n",
      "another specific feature of our work is that we connect is type correction explicitly with recently discovered ` exact approximation mcmc ' techniques @xcite .\n",
      "theoretical advances @xcite have already led to more efficient implementation of such methods , but has also revealed fundamental limitations .\n",
      "for instance , the methods may suffer from slow ( non - geometric ) convergence in practically interesting scenarios @xcite .\n",
      "recently proposed correlated version of the pseudo - marginal mcmc @xcite may help in more efficient implementation of pseudo - marginal type methods , but the question of their efficient parallelisability remains a challenge .\n",
      "the blocked parallelisable particle gibbs @xcite has appealing limiting properties , but its implementation still requires synchronisation between every update cycle , which may be costly in some computing environments .\n",
      "we recognise that our is approach comes with its own limitations ( see section [ sec : discussion ] ) , but we believe that it can often be very useful with parallel computing facilities .\n",
      "after preliminaries and notation in section [ sec : notation ] , we discuss a general importance sampling ( is ) type correction of mcmc outputs and we state related consistency results in section [ sec : simple ] .\n",
      "we detail the general case where unbiased estimators are used ( proposition [ prop : simple - is ] ) and then extend the setting in order to allow inference in an extended state space ( proposition [ prop : proper - consistency ] ) .\n",
      "the latter result is based on a general concept ( assumption [ a : proper ] ) , which we call ` proper weighting scheme ' following the terminology of liu @xcite .\n",
      "it accommodates naturally estimators from importance sampling and sequential monte carlo .\n",
      "we believe that assumption [ a : proper ] , or the more general assumption [ a : super - general ] , could be satisfied relatively easily in a wide variety of scenarios .    in section [ sec : ssm ] ,\n",
      "we discuss how our importance sampling correction method applies in general state space models ( ssm ) , and observe that sequential importance sampling and particle filters both lead to proper weighting schemes ( corollary [ cor : proper - ssm ] ) .\n",
      "the unbiasedness property of particle filters observed in @xcite suggests a method of its own interest : a simple , general and parallelisable general particle ssm smoothing algorithm , with naturally available consistent confidence intervals ( proposition [ prop : particle - smooth - confidence ] ) .\n",
      "we are unaware of earlier works which explicitly state this potentially useful result , but we are mindful that it may be widely known by particle filter experts .\n",
      "section [ sec : asvar - clt ] discusses conditions under which our estimators admit a central limit theorem .\n",
      "the expressions for the asymptotic variance may be useful , for instance , when optimising the computational resources .\n",
      "section [ sec : block ] focuses on a so - called jump chain representation @xcite , where we propose block estimators that use the jump chain structure . in this context , it is possible to employ variance reduction techniques , such as stratification or rao - blackwellisation using the scheme of @xcite . section [ sec : delayed ] starts by making an explicit connection between the proper weighting schemes and the pseudo - marginal type mcmc , and then between the is type correction of mcmc and a two - stage delayed acceptance scheme @xcite .    in section [ sec : exp ] , we detail a more specific class of ssms with linear - gaussian state dynamics and explain a generic laplace approximation method that fits this class of models . we compare different algorithmic variations with a model having poisson observations and with a stochastic volatility model . we conclude in section [ sec : discussion ] with a discussion of the possible implications of our findings .\n",
      "throughout the paper , we consider general state spaces while using standard integral notation . if the model at hand is defined on euclidean space using standard probability densities , the following paragraph can be skipped .\n",
      "each space @xmath0 is assumed to be equipped with a @xmath1-finite dominating measure ` @xmath2 ' on a @xmath1-algebra denoted with a corresponding calligraphic letter , such as @xmath3 .\n",
      "product spaces are equipped with the related product @xmath1-algebras and product dominating measures .\n",
      "if @xmath0 is a subset of an euclidean space @xmath4 , @xmath2 is taken by default as the lebesgue measure and @xmath3 as the borel subsets of @xmath0 .\n",
      "if @xmath5 is a probability density on @xmath0 , we denote the support of @xmath5 as @xmath6 , and the probability measure corresponding to @xmath5 with the same symbol @xmath7 .\n",
      "if @xmath8 , we denote @xmath9 , whenever well - defined . for a probability density or measure @xmath5 on @xmath0 and @xmath10 ,\n",
      "we denote by @xmath11 the set of measurable @xmath8 with @xmath12 , and by @xmath13 the corresponding set of zero - mean functions .\n",
      "if @xmath14 is a markov transition probability , we denote the probability measure @xmath15 , and the function @xmath16 .\n",
      "iterates of transition probabilities are defined recursively through @xmath17 for @xmath18 .\n",
      "we follow the convention @xmath19 , and if @xmath20 , we denote by @xmath21 the integers within the interval @xmath22 $ ] .\n",
      "we use this notation in indexing , so that @xmath23 , @xmath24 .\n",
      "if @xmath25 , then @xmath26 or @xmath27 is void , so that for example @xmath28 means @xmath29 .\n",
      "similarly , if @xmath30 is a vector , then @xmath31 and @xmath32 . we also use double - indexing , and write @xmath33 .\n",
      "the markov chain @xmath34 on @xmath0 is harris recurrent with invariant probability @xmath5 , if for every @xmath35 , and every initial distribution , the ergodic averages are strongly consistent , @xmath36 we call such a chain _ harris ergodic ( with respect to @xmath5)_.    recall that virtually all mcmc schemes are harris ergodic ( cf .\n",
      "* ) , although in some cases careless implementation could lead to non - harris chains ( cf .\n",
      "hereafter , @xmath37 is a probability density on @xmath0 and represents an approximation of a probability density @xmath38 of interest .\n",
      "[ a : mcmc - is ] the density @xmath37 and the related markov chain @xmath39 satisfy    a.   @xmath34 is harris ergodic with respect to @xmath37 .\n",
      "b.   [ item : support ] @xmath40 . c.   @xmath41 , where @xmath42 is a constant .\n",
      "assumption [ a : mcmc - is ] is equivalent to a seemingly more general statement , where @xmath38 and @xmath37 are probability measures with @xmath43 . in particular , taking @xmath37 as the dominating measure on @xmath0 , then one can define @xmath44 , and the density of @xmath38 is the corresponding radon - nikodym derivative @xmath45 .\n",
      "if assumption [ a : mcmc - is ] holds and it is possible to calculate the unnormalised importance weight @xmath46 pointwise , the chain @xmath39 can be weighted in order to approximate @xmath47 for every @xmath48 , using ( self - normalised ) importance sampling ( e.g. * ? ? ?\n",
      "* ; * ? ? ?\n",
      "* ) @xmath49{n\\to\\infty } \\frac{\\pi_a(w_u f)}{\\pi_a(w_u ) }      = \\pi(f ) ,      \\label{eq : mcmc - is}\\ ] ] as the harris ergodicity guarantees the almost sure convergence of both the numerator and the denominator .    in many cases of interest ,\n",
      "the importance weights @xmath46 can not be calculated or they are extremely costly to evaluate . instead , it is often possible to construct unbiased estimators of @xmath50 , which may be used in place of @xmath50 under mild conditions . in order to formalise such a setting ,\n",
      "we formulate the following abstract setting , which accommodates further generalisations of importance sampling type corrections .\n",
      "[ a : aug ] suppose @xmath34 is a harris ergodic markov chain on @xmath0 with invariant probability @xmath37 , and @xmath51 are conditionally independent given @xmath34 taking values on a space @xmath52 , such that the distribution of @xmath53 depends only upon the value of @xmath54 : @xmath55    in what follows , we consider several such augmented chains .\n",
      "note that the mapping @xmath56 in is never needed in practice , but only introduced for theoretic purposes .\n",
      "technically , @xmath56 is required to be a regular conditional distribution of @xmath53 given @xmath54 , but in practice , the concerns about the regularity of @xmath56 are usually unnecessary .\n",
      "[ thm : aug - consistency ] suppose assumption [ a : aug ] holds .\n",
      "then , @xmath57 is a harris ergodic markov chain on @xmath58 with invariant probability @xmath59    theorem [ thm : aug - consistency ] follows from lemma [ lem : aug - prop ] in appendix [ app : aug ] .\n",
      "let us now consider the simple importance sampling type correction where @xmath50 are replaced with unbiased estimates @xmath60 .\n",
      "this idea has been suggested in more specific contexts earlier at least in @xcite .    [ a : unbiased ] suppose assumption [ a : mcmc - is ] holds , @xmath61 are independent random variables on @xmath62 conditional on @xmath34 , the distribution of @xmath60 depends only on the value of @xmath54 , and satisfies @xmath63 { \\overset{\\mathrm{a.s.}}{=}}w_u(x)$ ] for @xmath37-almost every @xmath64 .    [ prop : simple - is ] suppose assumption [ a : unbiased ] holds and denote @xmath65 the estimator @xmath66 is strongly consistent , @xmath67 almost surely , if either of the following hold :    a.   @xmath68 almost surely and @xmath48 .\n",
      "b.   @xmath8 is such that @xmath69 , where @xmath70 $ ] .\n",
      "the results follow from theorem [ thm : aug - consistency ] , once we check that the functions @xmath71 and @xmath72 .    in proposition [ prop : simple -\n",
      "is ] :    a.   the case @xmath68 coincides with the case where pseudo - marginal algorithms can be implemented , and provide similarly consistent estimators for all @xmath48 ; see proposition [ prop : proper - pseudo ]\n",
      ". b.   when @xmath60 are allowed to take also negative values , we believe that @xmath66 might fail to be consistent for all @xmath48 , and an extra condition such as stated might be indeed necessary . c.   if the relative error in @xmath60 is bounded : there exists @xmath73 such that @xmath74 for all @xmath64 , then @xmath66 is guaranteed to be strongly consistent for all @xmath48 .\n",
      "we then focus on our main scenario , where both @xmath50 and @xmath75 may be estimated .\n",
      "such a scenario occurs when @xmath37 is approximation of a marginal density , which we discuss in detail after we state the following abstract assumption .\n",
      "[ a : super - general ] suppose assumption [ a : mcmc - is ] holds , let @xmath48 and let @xmath76 be @xmath77-valued random variables conditionally independent given @xmath34 , such that the distribution of @xmath53 depends only on the value of @xmath54 , and satisfies @xmath78 = f(x)w_u(x)\\qquad\\text{and}\\qquad      { \\mathbb{e}}[w_k\\mid x_k = x ] = w_u(x),\\ ] ] for @xmath37-almost every @xmath64 , and such that @xmath79\\ ] ] satisfies @xmath80 .\n",
      "[ prop : super - general ] if assumption [ a : super - general ] holds , then @xmath81{n\\to\\infty } \\pi(f).\\ ] ]    follows from theorem [ thm : aug - consistency ] , because both @xmath82 and @xmath83 are in @xmath84 .    in the specific cases we consider later , we define @xmath85 , where @xmath86 are random variables which satisfy @xmath87 = c_w \\pi(x ) f(x ) ] \\qquad \\text{and}\\qquad { \\mathbb{e}}[u_k\\mid x_k = x ] = c_w \\pi(x).\\ ] ] in some cases , however , it may be that @xmath88 is unavailable , for instance if @xmath89 is a pseudo - marginal algorithm targeting @xmath37\n",
      ". it may still be possible to estimate the ratio @xmath46 in an unbiased fashion .    in some scenarios\n",
      ", it may be possible to use simply @xmath90 which are independent of @xmath60 ( or @xmath91 ) , with @xmath92=f(x)$ ]\n",
      ". however , such @xmath90 may be more difficult to construct than dependent @xmath93 which satisfy assumption [ a : super - general ] .\n",
      "it is clear that assumption [ a : unbiased ] and the assumptions in proposition [ prop : simple - is ] form a special case of assumption [ a : super - general ] , by taking @xmath94 .\n",
      "the following scheme formulates a case where @xmath34 targets @xmath88 , which is an approximation of a marginal density @xmath95 , and the aim is inference over a joint target density @xmath96 on an extended state space @xmath97 .\n",
      "the following assumption is similar to ( * ? ? ?\n",
      "* definition 2.5.1 ) :    [ a : proper ] suppose assumption [ a : mcmc - is ] holds , @xmath98 and @xmath99 are conditionally independent given @xmath34 , and take values on @xmath100^m\\times{\\mathbb{r}}$ ] with @xmath101 for all @xmath102 , and such that for every @xmath103 , the random variables @xmath104 satisfy @xmath105 { \\overset{\\mathrm{a.s.}}{=}}w_u(x ) f^*(x)$ ] , where @xmath106 for @xmath37-almost every @xmath64 .\n",
      "[ prop : proper - consistency ] suppose that assumption [ a : proper ] holds .\n",
      "then , the estimator @xmath107 satisfies @xmath108 almost surely , if either of the following hold :    a.   [ item : proper - positive ] @xmath68 almost surely for all @xmath109 and @xmath103 . b.   [ item : proper - general ] @xmath103 and @xmath110\\ ] ] satisfies @xmath80 .\n",
      "this is a special case of proposition [ prop : super - general ] , because assumption [ a : proper ] together with , which is also implied by , implies assumption [ a : super - general ] .\n",
      "the following immediate result records that importance sampling estimators fit assumption [ a : proper ] naturally .\n",
      "[ prop : augmented - is ] suppose @xmath111 such that @xmath112 is a probability density for each @xmath64 and such that @xmath113 .\n",
      "let @xmath114 and denote @xmath115 where @xmath42 is some constant , and let @xmath116 .\n",
      "then , @xmath117 form a proper weighting scheme ( assumption [ a : proper ] ) , and therefore , for any @xmath103 , @xmath118{n\\to\\infty } \\pi^*(f).\\ ] ]    the next result shows that sub - sampling within a proper weighting scheme leads to a proper weighting scheme .\n",
      "[ prop : randomise - proper ] suppose that @xmath117 forms a proper weighting scheme .\n",
      "let @xmath119 be random variables conditionally independent of @xmath120 such that @xmath121 .\n",
      "then , @xmath122 forms a proper weighting scheme .    proposition [ prop : randomise - proper ] follows by observing that @xmath123 is a proper weighting scheme , with @xmath124 .\n",
      "proposition [ prop : randomise - proper ] implies that the estimator @xmath125{n\\to\\infty } \\pi^*(f)\\ ] ] under the conditions stated in proposition [ prop : proper - consistency ] .\n",
      "such extra randomisation generally leads to a higher asymptotic variance , but may be preferable if the weighted sample @xmath126 needs to be stored for further analysis .    in case of non - negative weights ,\n",
      "it is easy to see that the following convex combinations of proper weighting schemes remain proper .\n",
      "[ prop : convex - proper ] suppose @xmath127 and @xmath128 forms a proper weighting scheme for each @xmath129 , @xmath130 are conditionally independent given @xmath34 in the sense of assumption [ a : aug ] , and @xmath131 almost surely for all @xmath132 .\n",
      "then , the random variables @xmath133 form a proper weighting scheme @xmath134 for every @xmath135^n$ ] with @xmath136 .\n",
      "we discuss the implication of proposition [ prop : augmented - is ] in case of state space models in section [ sec : ssm ] .\n",
      "in such a setting , @xmath89 stands for the ( static ) parameters of the model , @xmath137 for the sampled latent state trajectories , and @xmath138 for the corresponding ( unnormalised ) importance weights .\n",
      "we recall also the unbiasedness property of particle filters which leads to proper weighting schemes .\n",
      "state space model ( ssm ) techniques are applied in a wide variety of applications ( cf . * ? ? ?\n",
      "we assume the existence of a family of ssms indexed by a static parameters @xmath139 taking values on a space @xmath140 .\n",
      "the model consists of a sequence of latent ( unobserved ) states @xmath141 taking values in @xmath142 , and observations @xmath143 taking values in @xmath144 .\n",
      "more specifically , the latent states @xmath141 form a markov chain with initial density @xmath145 and state transition densities @xmath146 for @xmath147 .\n",
      "the observations @xmath143 are conditionally independent given @xmath141 and follow the observation model @xmath148 .\n",
      "this model defines the following probability density on @xmath149 : @xmath150 where , by convention , @xmath151 .\n",
      "the inference is based on the so - called smoothing distribution @xmath152 where @xmath153 is a normalising constant .    in a bayesian state space model ,\n",
      "the parameters @xmath139 are regarded random as well , with a prior density @xmath154 , which leads to the following joint density on @xmath155 : @xmath156 the ( full ) bayesian inference is based on the posterior distribution @xmath157 with overall normalising constant @xmath158 .\n",
      "let us start with a description of a sequential importance sampling procedure , which requires user - supplied ` proposal ' distributions @xmath159 for @xmath160 .\n",
      "[ alg : sis ] input : @xmath143 and @xmath161 .\n",
      "a.   for @xmath160 , sample @xmath162 and calculate @xmath163    output : @xmath164 , where @xmath165 and @xmath166 .\n",
      "if the following well - known support condition holds , we recall that algorithm [ alg : sis ] provides a weighting which is shown to lead to proper weighting in corollary [ cor : proper - ssm ] .\n",
      "[ a : support ] suppose that @xmath167 for all @xmath160 , where @xmath168    [ prop : sis - proper ] suppose assumption [ a : support ] holds , then the random variables @xmath169 , @xmath170 and @xmath171 produced by algorithm [ alg : sis ] satisfy @xmath172      & = p^{(\\theta)}(y_{1:t } ) \\int p^{(\\theta)}(\\alpha_{1:t}\\mid y_{1:t } )      f(\\alpha_{1:t } ) { \\mathrm{d}}\\alpha_{1:t}.      \\label{eq : ssm - proper}\\end{aligned}\\ ] ] for any function @xmath173 with the above integral well - defined .\n",
      "proposition [ prop : sis - proper ] follows from proposition [ prop : augmented - is ] , because @xmath174 are ratios of @xmath175 and @xmath176 , which is the law of the independent trajectories @xmath177 .\n",
      "let us consider next a particle filter algorithm @xcite ; see also the monographs @xcite , which is often much more efficient in practice .\n",
      "the algorithm is based on same user - supplied proposal distributions @xmath178 as sis , but requires also a ` resampling ' distribution @xmath179 , which draws a sample @xmath180 from a discrete probability mass @xmath181 .\n",
      "whenever the index ` @xmath182 ' appears in the algorithm , it takes values @xmath183 .\n",
      "[ alg : pf ] input : @xmath143 and @xmath161 .\n",
      "a.   sample @xmath184 , set @xmath185 and calculate @xmath186    for @xmath147 , do    a.   sample @xmath187 , where @xmath188 b.   sample @xmath189 and set @xmath190 . c.   calculate @xmath191    if @xmath192 in algorithm [ alg : pf ] , the algorithm may be terminated immediately  all the estimators considered in proposition [ prop : particle - proper ] below equal to zero then .\n",
      "we prefer to avoid such explicit stopping mechanism for mathematical convenience .\n",
      "we require the following well - known unbiasedness condition on the resampling procedure which is satisfied by the common multinomial , stratified , residual and systematic resampling methods ( cf . * ? ? ?\n",
      "[ a : resampling ] the resampling procedure satisfies @xmath193=m\\bar{\\omega}^{(j)}.\\ ] ] for any @xmath194 and any probability mass vector @xmath181 .\n",
      "[ prop : particle - proper ] suppose that assumptions [ a : support ] and [ a : resampling ] hold , let @xmath161 and suppose @xmath173 is such that the integral in well - defined .\n",
      "consider the random variables generated in algorithm [ alg : pf ] , and let @xmath195 .\n",
      "a.   [ item : filter - smoother ] the random variables @xmath164 where @xmath196 and @xmath197 satisfy .\n",
      "define for @xmath147 , and any @xmath198 , the backwards sampling probabilities @xmath199 and in case the sum is zero , set @xmath200 .\n",
      "a.   [ item : backwards - sampling ] let @xmath201 be a random variable generated recursively by @xmath202 , and @xmath203 for @xmath204 .\n",
      "for any @xmath103 , the random variables @xmath205 satisfy , where @xmath206 and @xmath207 . b.   [ item : fwd - bwd - smoothing ] if @xmath103 and @xmath208 for some @xmath209 , that is , @xmath210 is constant in all coordinates except @xmath211 and @xmath212 , then , the random variables @xmath213 satisfy ( with @xmath214 ) , where a.   @xmath215 , b.   @xmath216 , and where c.   [ item : smoothing - weights ] @xmath217 and @xmath218 for @xmath219 . c.   [ item : fwd - bwd - smoothing2 ] if @xmath103 and @xmath220 for some @xmath221 , then the random variables @xmath222 satisfy ( with @xmath214 ) , where @xmath223 and @xmath224 are defined in .    we provide a self - contained but concise proof of proposition [ prop : particle - proper ] in appendix\n",
      "[ app : particle ] .\n",
      "proposition [ prop : particle - proper ]    * corresponds to so - called filter - smoother @xcite .\n",
      "this property was shown in ( * ? ? ?\n",
      "* theorem 7.4.2 ) , although only in case of multinomial resampling .\n",
      "the same property is behind the original version of the particle marginal metropolis - hastings @xcite ; see also remark [ rem : proper - mcmc ] .\n",
      "+ the statement holds also when the particle filter is applied with a general sequence of distributions @xmath225 rather than the ssm @xcite .\n",
      "* corresponds to backwards simulation , is analogous to the developments in conditional sequential monte carlo with backwards sampling @xcite .\n",
      "this allows us to draw any number of independent trajectories ( indices ) , and average them , in order to satisfy . * and its special case correspond to the forward - backward smoother @xcite\n",
      "; see also @xcite .\n",
      "it can be seen as rao - blackwellised version of , but is applicable only when @xmath210 is a specific form .\n",
      "this scheme can lead to lower variance , but it also has square complexity in @xmath226 , rendering it inapplicable with large @xmath226 .\n",
      "we next note how propositions [ prop : sis - proper ] and [ prop : particle - proper ] ensure that both algorithms [ alg : sis ] and [ alg : pf ] yield a proper weighting scheme .\n",
      "[ cor : proper - ssm ] let @xmath227 be a markov chain which is harris ergodic with respect to @xmath37 , and let @xmath228 correspond to    a.   independent runs of algorithm [ alg : sis ] with parameters @xmath227 , respectively , or b.   independent runs of a algorithm [ alg : pf ] with parameters @xmath227 , and the random variables defined in proposition [ prop : particle - proper ] or    then , the random variables @xmath229 with @xmath230 provide a proper weighting scheme for target distribution @xmath231 ( assumption [ a : proper ] ) .\n",
      "similarly , if @xmath228 correspond to    a.   [ item : pf - marginal - inference ] independent runs of a algorithm [ alg : pf ] with parameters @xmath227 , and the random variables defined in proposition [ prop : particle - proper ] or ,    then @xmath229 provide a proper weighting scheme for the marginal distribution @xmath232 or @xmath233 , respectively .    corollary [ cor : proper - ssm ] is stated for a single marginal ( pair ) , but it is clear that we may estimate simultaneously several marginal ( pairs ) , so that proposition [ prop : particle - proper ] applies for every function which is of the form @xmath234 and proposition [ prop : particle - proper ] whenever @xmath235 .\n",
      "it is possible to consider also more general classes of functions ; see for instance the discussion in @xcite .\n",
      "we next discuss one implication of proposition [ prop : particle - proper ] outside the main focus of this paper .\n",
      "namely , in the general state space smoothing context , that is , where @xmath161 is constant , proposition [ prop : particle - proper ] yields easily parallelisable particle smoothing algorithms , which also admit easily calculated consistent confidence interval estimates .\n",
      "[ prop : particle - smooth - confidence ] suppose @xmath161 is fixed , and let @xmath228 correspond to independent random variables as defined in proposition [ prop : particle - proper ] .\n",
      "a.   if the corresponding conditions of proposition [ prop : particle - proper ] are satisfied , then the estimator @xmath236{n\\to\\infty } \\mu_f { \\mathrel{\\mathop:}=}\\int      p^{(\\theta)}(\\alpha_{1:t}\\mid y_{1:t } ) f(\\alpha_{1:t}){\\mathrm{d}}\\alpha_{1:t},\\ ] ] where @xmath237 , whenever the integral is well - defined . b.   if also @xmath238\\big)^2\\big]<\\infty$ ] , then @xmath239 \\xrightarrow[\\mathrm{d}]{n\\to\\infty }      n(0,\\sigma^2 ) ,      \\qquad\\text{where}\\qquad      \\sigma^2 { \\mathrel{\\mathop:}=}\\frac{\\sigma_*^2}{p^{(\\theta)}(y_{1:t})^2}.\\ ] ] c.   if in addition @xmath240<\\infty$ ] , then @xmath241 , almost surely , where @xmath242\\bigg)^2.\\ ] ]    the result of proposition [ prop : particle - smooth - confidence ] is straightforward and probably widely known , but we highlight it here because of the practical consequences it has for parallel particle smoothing .\n",
      "a.   proposition [ prop : particle - smooth - confidence ] ensures that , under the given conditions , the confidence intervals @xmath243 $ ] are asymptotically consistent , where @xmath244 corresponds to the desirable standard gaussian quantile .\n",
      "b.   calculation of consistent confidence intervals for one realisation of a particle smoothing algorithm is not a straightforward task in general .\n",
      "another recently suggested method @xcite relies on unbiased estimators obtained by coupling of conditional sequential monte carlo chains and randomisation as suggested by @xcite . c.\n",
      "if the particle filter estimates satisfy a variance asymptotic such as @xmath245 where @xmath246 correspond to a particle filter run with @xmath226 particles , then if @xmath226 is taken sufficiently large , the estimate as in proposition [ prop : particle - smooth - confidence ] is nearly as efficient than the single particle filter estimate with @xmath247 particles , but benefits from easy parallel implementation .\n",
      "d.   even estimates which have quadratic cost ( in @xmath226 ) , as in proposition [ prop : particle - proper ] and , may be of practical interest in some scenarios . then , choosing an optimal @xmath226 leads to a non - trivial optimisation task .\n",
      "the asymptotic variance is a common efficiency criterion for markov chains .\n",
      "when a central limit theorem ( clt ) holds , the limiting variance coincides with the asymptotic variance .\n",
      "[ def : asvar ] suppose the markov chain @xmath34 on @xmath0 has transition probability @xmath14 which is harris ergodic with respect to invariant probability @xmath37\n",
      ". for @xmath248 , the asymptotic variance of @xmath210 with respect to @xmath14 is @xmath249\\bigg)^2,\\ ] ] whenever the limit exists in @xmath250 $ ] , where @xmath251 stands for the _ stationary markov chain _ with transition probability @xmath14 , that is , with @xmath252 .\n",
      "[ def : clt ] the markov chain @xmath34 satisfies a clt for @xmath210 , if @xmath253 , and for any initial distribution , @xmath254      \\xrightarrow{n\\to\\infty } n\\big(0,{\\mathrm{var}}(f , p)\\big),\\ ] ] in distribution . in case @xmath255 , the limit is a unit mass at zero .    in order to guarantee a clt for a markov chain\n",
      ", some form of rate of convergence is necessary .\n",
      "we follow jones @xcite and define the following :    [ def : convergence - rate ] suppose that a markov transition probability @xmath14 is harris ergodic with respect to a probability measure @xmath5 , and @xmath256 and @xmath257 are such that @xmath258 if @xmath259 , we say that @xmath14 admits @xmath260 convergence rate .\n",
      "different combinations of @xmath261 and @xmath262 imply a clt for different classes of functions ; see @xcite .\n",
      "we record the following summary of conditions which guarantee a clt for the augmented markov chain .\n",
      "the result follows easily from ( * ? ? ?\n",
      "* theorem 9 ) , given the following simple result whose proof can be found in appendix [ app : clt ] .\n",
      "[ lem : inherited - rate ] suppose that assumption [ a : aug ] holds , and the chain @xmath263 admits @xmath260 convergence rate .\n",
      "then , the chain @xmath57 admits a @xmath264 convergence rate , where @xmath265 .\n",
      "hereafter , we denote by @xmath266 the transition probability of @xmath267 and recall that @xmath268 is given in    [ prop : aug - clt ] suppose that assumption [ a : aug ] holds , and @xmath34 satisfies definition [ def : convergence - rate ] with some @xmath260 . if @xmath269 for some @xmath270 and if one of the following hold :    a.   [ item : uniform ] @xmath271 and @xmath34 is uniformly ergodic , that is , @xmath272 , b.   [ item : geometric ] @xmath273 for some @xmath274 and @xmath34 is geometrically ergodic , that is , definition [ def : convergence - rate ] holds with @xmath275 for some constants @xmath276 , c.   [ item : geometric - rev ] @xmath271 and @xmath34 is geometrically ergodic and reversible , d.   [ item : polynomial ] @xmath273 for some @xmath274 and @xmath34 is polynomially ergodic , @xmath277 with @xmath278 , and @xmath279 ,    then @xmath57 satisfies a clt for @xmath210 , with a finite limiting asymptotic variance @xmath280 where @xmath281 and @xmath282 are the conditional mean and variance , respectively : @xmath283    proposition [ prop : aug - clt ] is a consequence of lemma [ lem : inherited - rate ] , ( * ? ? ? * theorem 9 ) , and lemma [ lem : aug - prop ] and proposition [ prop : aug - asvar ] in appendix [ app : aug ] , because also reversibility of @xmath34 is inherited by @xmath57 .    if @xmath284 , then @xmath285 for all @xmath274 .\n",
      "it follows that assuming @xmath286 in is enough to guarantee a clt for @xmath287 .\n",
      "note that the @xmath288-geometric ergodicity can be shown to be inherited easily as well ( * ? ? ?\n",
      "* lemma 45 ) .\n",
      "let us finally apply these results to the estimator introduced in proposition [ prop : proper - consistency ] .    [\n",
      "prop : proper - clt ] suppose conditions of proposition [ prop : proper - consistency ] are satisfied , which guarantee the strong consistency of @xmath66 given in , and suppose @xmath34 satisfies one of the conditions in proposition [ prop : aug - clt ] with @xmath289 . denote @xmath290.\\ ] ] if @xmath291 , then @xmath292 \\xrightarrow{n\\to\\infty } n(0,\\sigma_f^2)\\ ] ] in distribution , where @xmath293 and where @xmath294 and @xmath295 .\n",
      "proof of proposition [ prop : proper - clt ] is given in appendix [ app : clt ] .\n",
      "the latter term in the asymptotic variance expression of proposition [ prop : proper - clt ] contains the contribution of the importance sampling noise .\n",
      "if the estimators @xmath296 are made increasingly accurate , in the sense that @xmath297 becomes negligible , the limiting case corresponds to an importance sampling corrected approximate mcmc , and evaluating the conditional expectation @xmath298 .\n",
      "we conclude this section by noticing that the contribution of the importance sampling correction in asymptotic variance as in proposition [ prop : proper - clt ] can be estimated in a straightforward manner .\n",
      "[ prop : importance - var ] suppose conditions of proposition [ prop : proper - consistency ] are satisfied , which guarantee the strong consistency of @xmath66 given in .\n",
      "suppose also that @xmath299 where @xmath300 is defined in proposition [ prop : proper - clt ] , and @xmath301 , where @xmath302 $ ] .\n",
      "then , the estimator @xmath303 satisfies @xmath304 almost surely as @xmath305 .\n",
      "proof of proposition [ prop : importance - var ] is given in appendix [ app : clt ] .\n",
      "the estimator @xmath306 in proposition [ prop : importance - var ] provides a _ lower bound _\n",
      "estimate for the clt variance @xmath307 of @xmath66 .\n",
      "it can provide useful information about the importance sampling noise contribution , and may be used as an optimisation criteria for the accuracy of the related estimators .\n",
      "estimation of the full asymptotic variance @xmath308 , which includes the marginal chain contribution @xmath309 , is not a straightforward task in general ; see for example @xcite and references therein .\n",
      "many mcmc algorithms include an accept - reject mechanism , which results in blocks of repeating values @xmath310 . when @xmath50 or @xmath311 are estimated , an obvious approach is to construct a single estimator which is used for the whole block .\n",
      "this may allow for variance reduction , for instance when replacing simple independent importance sampling with block - wide stratified estimator , or using particle filter estimators as in proposition  [ prop : particle - proper ] , and choosing the number of particles proportional to the block length .\n",
      "to formalise such an algorithm we consider the `` jump chain '' representation of @xmath34 ( cf .\n",
      "[ def : jump ] suppose that @xmath34 is a markov chain with non - degenerate transition probability @xmath14 , that is , @xmath312 for all @xmath64 .\n",
      "the jump chain @xmath313 with corresponding durations @xmath314 of @xmath34 is defined as follows : @xmath315 and @xmath316 , and then recursively @xmath317 where @xmath318 .\n",
      "we note the following observations regarding definition [ def : jump ] :    a.   if @xmath14 is harris ergodic with respect to @xmath37 which is non - trivial ( that is @xmath319 for any @xmath64 ) , then necessarily @xmath312 for all @xmath64 , so definition [ def : jump ] is consistent . b.   if @xmath34 corresponds to a metropolis - hastings chain , with non - diagonal proposal distributions @xmath320 ( that is , @xmath321 for every @xmath64 ) , then the jump chain @xmath322 is simply the accepted states , and @xmath323 is the number of rejections occurred at state @xmath322 .\n",
      "the importance sampling type estimator which we consider is of the following general form :    [ a : block - super - general ] suppose that assumption [ a : mcmc - is ] holds , let @xmath313 denote the corresponding jump chain ( definition [ def : jump ] ) , and assume @xmath48 .\n",
      "suppose that @xmath324 are conditionally independent random variables on @xmath77 given @xmath313 , whose distribution depends only on the value of @xmath325 , and which satisfy @xmath326      = \\frac{w_u(x)f(x)}{\\alpha(x)}\\qquad\\text{and}\\qquad      { \\mathbb{e}}[\\tilde{w}_k\\mid \\tilde{x}_k = x ] = \\frac{w_u(x)}{\\alpha(x)},\\ ] ] for @xmath37-almost every @xmath64 , and @xmath327\\ ] ] satisfies @xmath328 .\n",
      "[ prop : block - super - general ] suppose assumption [ a : block - super - general ] holds , then @xmath329{n\\to\\infty } \\pi(f).\\ ] ]    proposition [ prop : jump - properties ] implies that the jump chain @xmath322 corresponding @xmath89 is harris ergodic with invariant probability @xmath330 .\n",
      "the rest follows from proposition [ prop : super - general ] .\n",
      "we consider next the most obvious block estimator , based on @xmath314 , corresponding to the jump chain and assumption [ a : super - general ] .\n",
      "[ a : block - natural - general ] suppose that assumption [ a : mcmc - is ] holds , and let @xmath331 denote the corresponding jump chain ( definition [ def : jump ] ) .\n",
      "let @xmath332 correspond to a proper weighting scheme ( assumption [ a : proper ] ) , such that the variables in the scheme may depend on both @xmath325 and @xmath333 , and such that @xmath334      = w_u(x)f^*(x),\\ ] ] for all @xmath103 and @xmath37-almost every @xmath64 .\n",
      "[ prop : block - natural - general ] suppose assumption [ a : block - natural - general ] holds , and @xmath335      \\quad      \\text{satisfies }      \\quad      \\pi_a(b)<\\infty .\n",
      "\\label{eq : block - consistency - required}\\ ] ] then , @xmath336{n\\to\\infty } \\pi^*(f ) .\n",
      "\\label{eq : block - natural}\\ ] ]    this follows from proposition [ prop : block - super - general ] , because the holding times @xmath337 are conditionally independent on @xmath322 and geometric with parameter @xmath338 , with expectation @xmath339 = 1/\\alpha(x)$ ] .\n",
      "therefore , @xmath340 and @xmath341 satisfy assumption [ a : block - super - general ] .    regarding assumption [ a : block - natural - general ] :    a.   the estimators\n",
      "@xmath342 and @xmath60 which satisfy assumption [ a : block - natural - general ] may often come from a proper weighting scheme as in assumption [ a : proper ] , but here the parameter @xmath226 is allowed to depend on @xmath343 .\n",
      "b.   the condition on @xmath344 $ ] is not optimal .\n",
      "for instance , it is easy to find examples where the estimator is strongly consistent , even though the conditional expectation is unbounded in @xmath345 . in practice , however , the estimators are usually chosen either as independent of @xmath345 , or increasingly accurate in @xmath345 ; see remark [ rem : block - varying - or - not ] . c.   the estimator of proposition [ prop : augmented - is ] coincides with a block estimator where @xmath346 . however , the block estimator offers more flexibility . for instance\n",
      ", @xmath347 could be stratified , leading to a smaller variance .\n",
      "d.   even though we believe that the estimators of the form are often appropriate , we note that general form of the estimator as given in assumption [ a : block - super - general ] may be useful in some cases , in order to further reduce variance . for instance , assumption [ a : block - super - general ] accommodates the case where rao - blackwellised lower - variance estimators of @xmath348 are used instead of @xmath349 , as suggested in @xcite .\n",
      "let us then consider the asymptotic variance of the estimator .\n",
      "[ thm : block - clt ] suppose assumption [ a : block - natural - general ] holds , consider the estimator @xmath66 in and assume holds , @xmath350 , where @xmath351 , and @xmath352 , where @xmath353.\\ ] ] if @xmath34 is @xmath288-geometrically ergodic , then @xmath354 \\xrightarrow{n\\to\\infty } n(0,\\sigma^2 )         \\qquad\\text{in distribution},\\ ] ] where the limiting variance can be given as : @xmath355 ,      \\label{eq : block - clt - var}\\ ] ] where @xmath356,\\ ] ] and @xmath14 stands for the markov transition probability of @xmath34 .\n",
      "proof of theorem [ thm : block - clt ] is given in appendix [ app : jump ] .\n",
      "[ rem : block - varying - or - not ] in theorem [ thm : block - clt ] asymptotic variance expression , the latter term corresponds to the contribution of noise in importance sampling estimates .\n",
      "a.   when @xmath60 and @xmath357 correspond to @xmath333 independent estimates and @xmath358 , we might have @xmath359 , which leads to @xmath360 , so the contribution of the is would be upper bounded by @xmath361 .\n",
      "b.   similarly , if @xmath362 , which could occur when @xmath93 are independent of @xmath333 , then , @xmath363 and the contribution of is would be upper bounded by @xmath364 .\n",
      "the acceptance probability is lower bounded for geometrically ergodic chains @xcite , rendering the estimator a safe choice .    about the assumption of @xmath288-geometric ergodicity in theorem [ thm : block - clt ] :    a.   the asymptotic variance expression we derive for the ` numerator ' part of our estimator holds whenever @xmath34 is reversible , due to a result of deligiannidis and lee @xcite .\n",
      "this does not , however , imply a clt .\n",
      "b.   our proof relies on the existence of a solution @xmath365 to the poisson equation @xmath366 .\n",
      "we believe that the jump chain @xmath313 inherits a central limit theorem from the base chain @xmath34 under general conditions , similarly as in section [ sec : asvar - clt ] .\n",
      "however , the jump chain @xmath313 does not inherit a convergence rate from @xmath34 in general , so a simple approach such as in section [ sec : asvar - clt ] does not apply .\n",
      "we start by making an explicit connection of proper weighting schemes with positive weights and pseudo - marginal type mcmc algorithms .\n",
      "[ a : proper - pseudo ] suppose that @xmath367 is a probability density on @xmath97 , @xmath368 is a family of proposal densities on @xmath0 and @xmath369 is a family of probability distributions on @xmath370^m\\times[0,\\infty)$ ] such that for almost every @xmath64 and all @xmath103 , the variables @xmath371 satisfy @xmath372      = \\varpi(x ) \\int \\pi^*(z\\mid x )      f(x , z ) { \\mathrm{d}}z,\\ ] ] where @xmath373 .\n",
      "[ alg : proper - pseudo ] suppose assumption [ a : proper - pseudo ] holds , let @xmath374 and @xmath375 and for @xmath102 iterate :    a.   draw @xmath376 and @xmath377 .\n",
      "b.   with probability @xmath378 accept and set @xmath379 ; otherwise reject and set @xmath380 .\n",
      "[ prop : proper - pseudo ] consider algorithm [ alg : proper - pseudo ] .\n",
      "if @xmath381 is integrable and the markov chain @xmath57 is harris recurrent , then @xmath382{n\\to\\infty } \\pi^*(f)\\qquad\\text{for        every $ f\\in l^1(\\pi^*)$,}\\ ] ] if and only if assumption [ a : proper - pseudo ] holds with @xmath383 .    if @xmath381 is integrable\n",
      ", then @xmath384 defines an unnormalised probability on @xmath385 .\n",
      "define a composite proposal distribution @xmath386 , then it is straightforward to check that @xmath387 that is , algorithm [ alg : proper - pseudo ] is a metropolis - hastings with target @xmath388 , and therefore reversible with respect to @xmath389 .\n",
      "if the chain is harris ergodic , then for every @xmath390 , the strong law of large numbers holds .\n",
      "[ rem : proper - mcmc ] proposition [ prop : proper - pseudo ] connects proper weighting explicitly with the validity of pseudo - marginal type algorithms .\n",
      "this analogue also suggests that our is type correction may be applied instead of any pseudo - marginal method .\n",
      "the proper weighting formulation may also suggest variations of pseudo - marginal schemes .\n",
      "for example , proposition [ prop : convex - proper ] suggests about the possibility of using several independent particle filters in a particle marginal metropolis - hastings update , which may be of interest in parallel computing .\n",
      "let us then focus on delayed acceptance ( da ) mcmc @xcite , which is a multiple - stage acceptance method in metropolis - hastings algorithm , which is often based on an approximate target distributions in order to reduce computational effort of a metropolis - hastings algorithm .\n",
      "we focus in particular on the two - stage da scheme based on an approximation @xmath37 of the true probability density @xmath38 , which has obvious similarities with the importance sampling scheme suggested earlier .\n",
      "such da scheme with proposal densities @xmath368 can be summarised as follows :    [ alg : da ] suppose @xmath391 and for @xmath102 ,    a.   [ item : proposal ] generate a proposal @xmath392 b.   [ item : first - stage ] with probability @xmath393 continue to step , otherwise reject and set @xmath394 . here , @xmath395 c.   [ item : second - stage ] with probability @xmath396 set @xmath397 , otherwise reject and set @xmath398 .\n",
      "the steps and alone would implement a regular metropolis - hastings step targeting @xmath37 , whereas the second - stage acceptance step alone would implement the accept - reject step of independence metropolis - hastings algorithm with proposal @xmath37 .\n",
      "the rationale behind the da scheme is based on the same two assumptions which make the importance sampling corrected mcmc appealing : the evaluation of @xmath37 costs less than that of @xmath38 , and @xmath37 approximates @xmath38 reasonably well .\n",
      "the former directly affects the expected cost of the algorithm : if @xmath399 is rejected at step , there is no need to calculate @xmath400 , implying lower expected cost .\n",
      "the latter assumption guarantees that the second - stage acceptance rate is typically close to one .\n",
      "even though the da version is guaranteed to be worse than direct metropolis - hastings targeting @xmath38 in the peskun sense @xcite , the lower cost often outweighs the difference by allowing a greater number of samples to be drawn with da .\n",
      "we note that a proper weighting scheme ( assumption [ a : proper ] ) with non - negative weights leads to a valid da scheme .    [ a : da - proper ] suppose assumption [ a : proper - pseudo ] holds with @xmath401 and a @xmath37 is a probability density on @xmath0 with @xmath40 .\n",
      "[ alg : da - proper ] suppose assumption [ a : da - proper ] holds , and @xmath402 are proposal densities as in algorithm [ alg : da ] .\n",
      "define the markov chain @xmath267 with initial values @xmath403 and @xmath404 , and for @xmath102 as follows :    a.   generate a proposal @xmath392 .\n",
      "b.   [ item : first - stage2 ] with probability @xmath393 continue to step , otherwise reject and set @xmath405 . c.   [ item : second - stage2 ]\n",
      "generate @xmath406 , then with probability @xmath407 accept and set @xmath408 , otherwise reject and set @xmath380 .    [\n",
      "prop : da - valid ] if assumption [ a : da - proper ] holds and @xmath402 are such that algorithm [ alg : da - proper ] defines a harris recurrent markov chain @xmath57 , then algorithm [ alg : da - proper ] satisfies @xmath409{n\\to\\infty } \\pi^*(f ) ,      \\qquad\\text{for every $ f\\in l^1(\\pi^*)$.}\\ ] ]    the proof of proposition [ prop : da - valid ] follows from @xcite , because algorithm [ alg : da - proper ] is a valid da version of algorithm [ alg : proper - pseudo ] .\n",
      "proposition [ prop : da - valid ] highlights that when a da - mcmc scheme is of the pseudo - marginal form as algorithm [ alg : da - proper ] , the importance sampling type correction as we suggest is applicable , and a natural alternative for da - mcmc .\n",
      "the efficiency of da - mcmc was studied by banterle , grazian , lee and robert @xcite .\n",
      "sherlock , thiery and golithly @xcite studied the efficiency of pseudo - marginal with da as in algorithm [ alg : da - proper ] , with diffusion limit techniques .\n",
      "contrary to the is scheme , it is much harder to study the effect of noisy importance weights with da in general , as the noise affects the markov chain convergence rate .\n",
      "however , the authors @xcite are able to provide useful heuristic how to tune the accuracy of the pseudo - marginal da method .\n",
      "we now turn into an illustration of our generic framework with more specific state space models .\n",
      "section [ sec : lin - gauss - ssms ] discusses an important sub - class of state space models , where the state dynamics are linear - gaussian .\n",
      "we describe a general method which is based on a gaussian approximation of the conditional smoothing distribution .\n",
      "a special case of the general state space model introduced in section [ sec : ssm ] is a model where both @xmath142 and @xmath144 are euclidean , and the state dynamics @xmath410 are linear - gaussian , but the observational distribution @xmath411 may be non - gaussian and/or non - linear . more specifically , @xmath412 are defined through @xmath413 and @xmath414 our approach allows , for instance , exponential family observation models with gaussian , poisson , binomial , negative binomial , and gamma distributions , and a stochastic volatility model .\n",
      "we discuss the specific models later in sections [ poisson ] and [ sv - model ] .\n",
      "compared to the general state space form , this class of models may seem restrictive , but it still contains a large number of commonly used statistical models , such as ( generalised ) structural time series models , cubic splines , generalised linear models , and autoregressive integrated moving average ( arima ) models .\n",
      "the benefit of working within this subset of general state space models is that we have access to approximations taking advantage of the specific structure of the model .\n",
      "our approximation scheme , based on @xcite , relies on a laplace approximation @xmath415 of the conditional densities @xmath416 .\n",
      "the approximating gaussian model is found by using an iterative process closely related to iterative reweighted least squares algorithm used in generalised linear models context @xcite . in the approximating gaussian model\n",
      ", the observation equation is replaced by a linear - gaussian one @xmath417 where the pseudo - observations @xmath418 and their variances @xmath419 are based on the first and second derivatives of @xmath420 with respect to @xmath421 at @xmath422 , the conditional mode estimate of the approximate model @xcite .\n",
      "our approximation of the marginal density @xmath37 is based on the marginal likelihood decomposition @xmath423,\\ ] ] where @xmath424 , the terms @xmath425 correspond to the product of pseudo - observation densities , and the expectation is taken with respect to @xmath426 .\n",
      "if the approximation @xmath427 is close to @xmath428 in the high - probability regions of @xmath426 , the expectation term is close to one , so we take @xmath429 our approximation defines , in fact , a joint approximate distribution @xmath430 , and we use @xmath431 as a proposal distribution in our importance sampling correction .\n",
      "we use the laplace approximation @xmath432 described in section [ sec : lin - gauss - ssms ] as the importance distribution , and we use the simulation smoothing algorithm @xcite to simulate antithetic pairs of @xmath141 from @xmath432 .\n",
      "we implemented the following inference algorithms in our experiments :    amcmc : :    approximate mcmc targeting @xmath433 , and for each    accepted @xmath139 , sampling one realisation from    @xmath434 with the    simulation smoother .\n",
      "mcmc : :    exact pseudo - marginal mcmc ( algorithm [ alg : proper - pseudo ] ) algorithm    using importance sampling estimates with @xmath226 samples .\n",
      "da : :    delayed acceptance version of the above ( algorithm [ alg : da - proper ] ) .\n",
      "is1 : :    simple importance sampling estimator given in with @xmath226    samples .\n",
      "is2 : :    blocked estimator as in , with is estimators based on @xmath226    samples , independent of @xmath435 . is - pf : :    this is similar to is2 but uses the bootstrap particle filter @xcite    ( algorithm [ alg : pf ] ) with @xmath226 samples , and with stratified    sampling @xcite .\n",
      "the filter - smoother estimate ( proposition    [ prop : particle - proper ] ) was used .    in all the mcmc methods , the adaptive random walk metropolis algorithm of @xcite with gaussian proposal distribution was used . the target mean acceptance rate\n",
      "was set to 0.234 . in the delayed acceptance ,\n",
      "the first - stage acceptance rate was optimised , and in the pseudo - marginal , the total acceptance rate was optimised . in all cases ,\n",
      "the adaptive phase was terminated after the burn - in phase .\n",
      "all the experiments were conducted using the ` bssm ` @xcite package in ` r ` @xcite , which will be made available .\n",
      "we also experimented with the following algorithmic modifications :    global approximation : :    in the standard version , a laplace approximation is constructed for    every new proposed value of @xmath139 .\n",
      "we experimented also    faster but less accurate alternative , where we consider only one    series of pseudo - observations constructed at the maximum likelihood    estimate of @xmath139 .\n",
      "parallel computation : :    we tested our implementation with different number of cores .\n",
      "the    number of cores used in the experiment is indicated in parenthesis    such as is2 ( 8) .\n",
      "sub - sampling for memory constraints : :    depending on the problem , we might be interested in either computing    some given summary statistics of @xmath141 , which can    be performed recursively , or constructing a weighted sample from    @xmath436 . in order to save in    storage , we used sub - sampling as in proposition    [ prop : randomise - proper ] for the latter .    our main aim is to compare the performance of the da - mcmc with our is type correction schemes , using the same approximate marginal target distribution in both .\n",
      "note that , if parallel aspects are not taken into account , the is2 estimator has typically lower cost than da with same parameter @xmath226 . indeed ,\n",
      "if the second - stage proposal would always be accepted , then da would require approximately the same number of computations as is2\n",
      ". on the other hand , is1 has typically higher cost than da . because da usually has non - zero second - stage acceptance probability , da with @xmath226 samples has roughly the same cost as is1 using @xmath437 samples , where @xmath438 accounts for rejected second - stage proposals . because the noise in the weighting scheme affects the mixing of the mcmc and da , whereas is1 and is2 are based solely on the approximate marginal mcmc chain plus is type correction , the efficiency of the estimators is likely to be problem dependent .\n",
      "note also that mcmc and da methods require is steps also during the burn - in , whereas is1 , is2 and is - pf do not .\n",
      "we consider first the following special case of the model discussed in section [ sec : lin - gauss - ssms ] : @xmath439 with @xmath440 . for testing our algorithms\n",
      ", we simulated @xmath441 observations from this model with @xmath442 fixed to @xmath443 , and @xmath444 fixed to @xmath445 .    based on our pilot experiments , the pseudo - marginal type mcmc performed well with this model using global marginal likelihood approximation , so we employed it for the experiments .\n",
      "we also observed that the number of importance samples @xmath226 could be set relatively low without affecting the mixing or accuracy much .\n",
      "we ran amcmc , mcmc , da , is1 , and is2 algorithms with @xmath446 , using 20,000 mcmc iterations with the first half discarded as burn - in .\n",
      "we used a uniform prior @xmath447 for the standard deviation parameters , where the cut - off parameter @xmath448 were set to @xmath449 based on the sample standard deviation of @xmath450 , where zeros were replaced with 0.1 .\n",
      "results were not sensitive to this upper bound .\n",
      "we performed two experiments . in the first\n",
      ", we stored only one realisation of @xmath141 at each iteration , and the resulting samples was used to calculate the posterior mean and variance estimates of @xmath139 , @xmath451 , and @xmath452 . in the second experiment , we used all realisations from the simulation smoother and computed the posterior mean and variance estimates of @xmath451 and @xmath452 recursively .\n",
      "tables [ table : poisson ] and [ table : poisson_summary ] summarise the results over 1000 repeated experiments as discussed above . in table\n",
      "[ table : poisson ] , the approximate method gave slightly biased estimates for all variables , but with much less cost than exact mcmc .\n",
      "delayed acceptance decreased the computation costs by about two thirds , while the is2 ( with same @xmath226 and no parallelisation ) almost halves the costs of da . parallel computing with eight cores further decreased the computation time of is - corrected methods , but because of small @xmath226 , the speed - up was limited , as it is impossible to get much lower than with amcmc . using @xmath226 proportional to the block size ( is1 ) does not seem to bring significant benefits in this experiment .\n",
      "overall , the main differences in da and is corrected methods are related to computational time .\n",
      "the parameter estimates and their standard errors are comparable .\n",
      "the pseudo - marginal mcmc provided perhaps slightly smaller standard errors , but due to the much higher computational time , the da and is2 appear preferable in this scenario\n",
      ".    .means and the corresponding standard errors from 1000 mcmc runs for poisson model with 10 runs of simulation smoother per iteration , with one stored sample path . [ cols=\">,>,>,>,>,>,>\",options=\"header \" , ]      as an illustration of the estimators in proposition [ prop : particle - smooth - confidence ] , we simulated one realisation @xmath441 from a linear - gaussian ssm of the form @xmath453 with @xmath454 and @xmath455 . given @xmath441 , we estimated @xmath139 via maximum likelihood , resulting @xmath456 and calculated the exact smoothing means @xmath457 $ ] for @xmath458 .\n",
      "we performed particle smoothing using the filter - smoother ( fs ) and forward - backward smoother ( fbs ) ( proposition [ prop : particle - proper ] and , respectively ) , and repeated these computations @xmath459 times , from which we obtained 95% confidence intervals as in proposition [ prop : particle - smooth - confidence ] .\n",
      "figure [ fig : ci_plot ] shows confidence intervals from both methods with varying number of particles . for fbs , we used @xmath460 , and for fs , the corresponding squares @xmath461 , making the computational times roughly comparable .\n",
      "we note that in our implementation , fbs was about 1.4 times slower than fs on average .\n",
      "it appears that in this experiment , fs outperformed fbs with comparable computational cost .\n",
      "$ ] with @xmath462 , using forward - backward smoother algorithm ( fbs ) , and filter - smoother algorithm ( fs ) with varying number of particles ( note that algorithms use different amount of particles ) .\n",
      "red line corresponds to the exact value computed by kalman smoothing . ]    because fbs may be of interest in some other scenarios , we attempted to optimise the choice of @xmath226 for the fbs .\n",
      "we ran the fbs with varying choices of @xmath226 as above , and using @xmath463 , respectively .\n",
      "figure [ fig : fbs_time ] shows the square root of average variance estimates @xmath306 of proposition [ prop : particle - smooth - confidence ] over time points @xmath160 , multiplied by the square root of computation time in minutes for different @xmath226 . in this experiment , @xmath464 or @xmath465 appeared good choices , but more importantly , the experiment suggests that the choice of @xmath226 in fbs may be a non - trivial optimisation problem .    .\n",
      "we suggested to use mcmc targeting a marginal of the posterior distribution and consequent importance sampling type estimation in order to perform easily parallelisable consistent inference over the full posterior distribution .\n",
      "we focused on state space models ( ssms ) , where importance sampling and particle filters provide natural and efficient correction mechanisms .\n",
      "our specific application was based on a laplace approximation of ssms with linear - gaussian state dynamics , but arbitrary observation distributions .\n",
      "our experiments are promising , and we believe that our methodology has potential to make exact inference feasible with a large class of models where only approximate inference is currently possible .\n",
      "alternative approximation methods in the state space context include simple linearisation - based methods such as the extended kalman filter , moment matching , or variational approximations ; see the recent monograph @xcite .\n",
      "gaussian latent variable models beyond the state space context could be handled with the laplace approximations , and more refined approaches such as the integrated nested laplace approximation @xcite could provide a useful approximate marginal .\n",
      "we note that , unlike with purely approximate inference , the approximate posterior @xmath37 does not need to be very accurate .\n",
      "we suggested importance ( is ) sampling type correction with sequential is and sequential monte carlo ( smc ) .\n",
      "we note that various smc variations , such as rao - blackwellised smc , and alternative resampling strategies , apply directly @xcite .\n",
      "randomised sequential quasi - monte carlo @xcite can be useful , when applicable .\n",
      "smc samplers can be useful beyond the state space context @xcite .\n",
      "the correction could be based on exact sampling for diffusions @xcite , and debiasing tricks and multilevel monte carlo @xcite can be useful for instance when the true target distribution is based on a continuum model , and only discretisations are accessible .\n",
      "importance sampling is known to be generally hard to implement efficiently in higher dimensions , and our setting is no exception .\n",
      "the fact that the approximation @xmath37 is required only on a marginal distribution may help , if the model admits a natural smaller dimensional parameter space , as our state space application .\n",
      "indeed , in the state space context our approach relies on similar assumptions which make particle mcmc @xcite appealing , with the added assumption of the existence of suitable marginal approximation @xmath37 .\n",
      "if such approximation is available , we suggest to take advantage of it , as an alternative to particle mcmc .    one potential issue of importance sampling correction , in general , is when the importance weight @xmath46 is unbounded . in our experiments\n",
      ", we did not observe large values of importance weights , but this is likely due to the fact that our space @xmath466 was bounded . in case @xmath0\n",
      "is unbounded , it may be that the ratio @xmath467 is unbounded , even if @xmath88 is a reasonable approximation in the ` centre ' of @xmath0 . because bounded @xmath468 is generally desirable ,\n",
      "we suggest tempering as a possible remedy .\n",
      "that is , if @xmath37 is our original approximate marginal , the chain @xmath34 may be designed to target @xmath469 instead , where @xmath470 is a tempering parameter .\n",
      "it may be of practical interest to construct the marginal chain @xmath34 using adaptive mcmc techniques such as @xcite or as discussed in the review @xcite .\n",
      "we note that our theory does not apply directly with adaptive mcmc , unless the adaptation is stopped after suitable burn - in , like we do in our experiments .\n",
      "an obvious alternative to our approach is that , instead of running one markov chain targeting @xmath37 and then calculating conditional importance sampling type estimates , one may consider running multiple independent pseudo - marginal type mcmc methods in parallel .\n",
      "we note that while valid , this approach may suffer from bias if the mcmc chains are not run long enough , and burn - in ( and possible adaptation ) of each chain could be seen wasteful .\n",
      "it is well - known that pseudo - marginal methods may be slow mixing when the importance weights have large variation  in the extreme case they may lose desirable rates of convergence such as geometric ergodicity @xcite .\n",
      "large variation of importance weights affects also our method , but we believe that mcmc targeting the approximate target @xmath37 may be often easier to design in an efficient manner .\n",
      "the authors were supported by an academy of finland research fellowship ( grants 274740 and 284513 ) .\n",
      "we thank christophe andrieu for insightful remarks .\n",
      "throughout this section , suppose that @xmath471 is a markov transition probability on @xmath0 and @xmath472 is a markov transition probability from @xmath0 to a space @xmath52 .\n",
      "we consider here properties of an augmented markov transition probability @xmath473 defined on @xmath58 as follows : @xmath474 we first state the following basic result .\n",
      "[ lem : aug - prop ] the properties of @xmath471 and the augmented chain @xmath473 are related as follows :    a.   [ lem : aug - mim - irr ] let @xmath475 denote the set of @xmath476-irreducibility measures of a markov transition probability @xmath471 , then @xmath477 b.   [ lem : aug - mim - mim ] the implications in hold when @xmath475 and @xmath478 are replaced with sets of maximal irreducibility measures of @xmath471 and @xmath473 , respectively . c.   [ item : aug - invar ] the invariant probabilities of @xmath471 and @xmath473 satisfy : @xmath479 the above implications hold also with invariance replaced by reversibility .\n",
      "d.   [ item : aug - harris ] @xmath471 is harris recurrent if and only if @xmath473 is harris recurrent .\n",
      "e.   [ item : aug - iterates ] suppose @xmath480 is measurable and such that @xmath481 and @xmath482 are well - defined\n",
      ". then , for any @xmath483 , @xmath484    the inheritance of irreducibility measures ( [ lem : aug - mim - irr ] ) , maximal irreducibility measures ( [ lem : aug - mim - mim ] ) , invariant measures , and reversibility are straightforward .    for harris recurrence ,\n",
      "let the probability @xmath485 be a maximal irreducibility measure for @xmath471 , then @xmath486 is the maximal irreducibility measure of @xmath473 .\n",
      "let @xmath487 with @xmath488 , and choose @xmath489 such that @xmath490 , where @xmath491 with @xmath492 .\n",
      "notice that @xmath493 where @xmath494 are the hitting times of @xmath89 to @xmath495 .\n",
      "this concludes the proof because @xmath496 are independent bernoulli random variables with success probability at least @xmath497 .\n",
      "the converse statement is similar .    for\n",
      ", it is enough to notice that for any @xmath498 and @xmath483 , it holds that @xmath499 .\n",
      "we next state the following generic result which gives an expression for the asymptotic variance of an augmented markov chain .\n",
      "[ prop : aug - asvar ] let @xmath500 and define the conditional mean @xmath501 and the conditional variance @xmath502 , then @xmath503 whenever @xmath504 is well - defined .\n",
      "let @xmath267 be the stationary markov chain with transition probability @xmath473 .\n",
      "@xmath505\\bigg ) \\\\      & = \\check{\\mu}(h^2 ) + \\frac{2}{n } \\sum_{i=1}^{n-1 }      \\sum_{\\ell=1}^{n - i } { \\mathbb{e}}[h(x_0,s_0)h(x_\\ell , s_\\ell)],\\end{aligned}\\ ] ] by stationarity .\n",
      "we may write by lemma [ lem : aug - prop ] , @xmath506      & = \\int \\check{\\mu}({\\mathrm{d}}x\\times { \\mathrm{d}}s ) h(x , s ) ( k^\\ell m_h)(x ) \\\\      & = { \\mathbb{e}}[m_h(x_0 ) m_h(x_\\ell)].\\end{aligned}\\ ] ] we deduce that @xmath507 where @xmath508 .\n",
      "the claim follows by taking limit @xmath305 .\n",
      "we finally state a simple result about the relationship of the solutions of the poisson equation for augmented chains , used in appendix [ app : jump ] .\n",
      "[ lem : aug - poisson ] suppose that @xmath500 .\n",
      "if there exists @xmath509 such that @xmath510 , where @xmath511 , then @xmath512    it is clear that @xmath513 and @xmath514 , so @xmath515\n",
      "we follow @xcite and prove the statement assuming that the resampling scheme @xmath516 satisfies the following property : @xmath517 for all @xmath518 .\n",
      "we can do this without loss of generality , because this condition would be satisfied if we applied an independent random permutation of indices to the resampling scheme satisfying assumption [ a : resampling ] .\n",
      "the statement is valid without such randomisation , because both the algorithm and the statements are independent of ordering .    for , define the filtrations @xmath519 and @xmath520 , and functions @xmath521 , and for @xmath522 @xmath523 clearly @xmath524 , and all @xmath525 are ( almost everywhere ) well - defined if the latter integral is well - defined .\n",
      "let us first observe that for @xmath147 , @xmath526      & =      { \\mathbb{e}}\\big [ { \\mathbb{e}}\\big [      \\omega_t^{(i ) }      f_t(\\bar{\\alpha}_{t-1}^{(a_{t-1}^{(i)})},\\alpha_t^{(i ) } )      { \\mathrel{\\big|}}{\\mathcal{g}}_{t-1 }      \\big ]      { \\mathrel{\\big|}}{\\mathcal{f}}_{t-1 }      \\big ]       \\label{eq : unbiased - step }       \\\\      & =      { \\mathbb{e}}\\big [      f_{t-1}(\\bar{\\alpha}_{t-1}^{(a_{t-1}^{(i ) } ) } )      { \\mathrel{\\big|}}{\\mathcal{f}}_{t-1 }      \\big ]       \\notag      \\\\      & =      \\sum_{i=1}^m \\bar{\\omega}_{t-1}^{(i ) }      f_{t-1}(\\bar{\\alpha}_{t-1}^{(i ) } ) .\n",
      "\\notag\\end{aligned}\\ ] ] by applying recursively , we obtain @xmath527      & = { \\mathbb{e}}\\bigg[\\bigg(\\prod_{t=1}^{t-1 } \\frac{1}{m}\\omega_t^ * \\bigg )       \\frac{1}{m }      \\sum_{i=1}^m       { \\mathbb{e}}\\big [ \\omega_t^ *   \\bar{\\omega}_t^{(i ) }      f_t(\\bar{\\alpha}_{t-1}^{(a_{t-1}^{(i)})},\\alpha_t^{(i)}){\\mathrel{\\big|}}{\\mathcal{f}}_{t-1}\\big]\\bigg ] \\\\ & = { \\mathbb{e}}\\bigg[\\bigg(\\prod_{t=1}^{t-1 } \\frac{1}{m } \\omega_t^*\\bigg ) \\sum_{i=1}^m       \\bar{\\omega}_{t-1}^{(i ) } f_{t-1}(\\bar{\\alpha}_{t-1}^{(i ) } )     \\bigg ] \\\\     & = \\frac{1}{m}\\sum_{i=1}^m          { \\mathbb{e}}\\big [ \\omega_1^ * \\bar{\\omega}_t^{(i ) }      f_1(\\alpha_1^{(i)})\\big],\\end{aligned}\\ ] ] which equals to @xmath528 by a similar calculation as in .\n",
      "the statement is equivalent with @xmath529 = \\int p^{(\\theta)}(\\alpha_{1:t},y_{1:t } ) f(\\alpha_{1:t } )      { \\mathrm{d}}\\alpha_{1:t}.      \\label{eq : all - trajectories - proper}\\ ] ] similarly as above , we have for @xmath522 , @xmath530 \\\\      & = \\int \\sum_{j=1}^n \\bar{\\omega}_{t-1}^{(j ) } \\mu_t(\\alpha_t\\mid      \\alpha_{t-1}^{(j ) } ) g_t(y_t\\mid \\alpha_t )      \\frac {        \\bar{\\omega}_{t-1}^{(i_{t-1 } ) }        \\mu_t(\\alpha_t\\mid \\alpha_{t-1}^{(i_{t-1 } ) } )         } {    \\sum_{\\ell=1}^m       \\bar{\\omega}_{t-1}^{(\\ell ) }       \\mu_t(\\alpha_t\\mid\\alpha_{t-1}^{(\\ell ) } ) }   f_t(\\alpha_{1:t}^{(i_{1:t } ) } ) { \\mathrm{d}}\\alpha_t \\\\ & = \\bar{\\omega}_{t-1}^{(i_{t-1 } ) } f_{t-1}(\\alpha_{1:t-1}^{(i_{1:t-1})}),\\end{aligned}\\ ] ] and application of this recursively leads to .\n",
      "the result follows also from , by first summing over the indices @xmath531 .\n",
      "the last result follows as a special case of .\n",
      "consider the artificial trivial markov chain @xmath89 with @xmath532 for @xmath102 which is harris ergodic with respect to @xmath533 , with @xmath534 , and let @xmath535 .\n",
      "then , @xmath536 and @xmath537 with @xmath538 forms a proper weighting scheme for @xmath539 .\n",
      "the strong consistency follows from proposition [ prop : proper - consistency ] and the central limit theorem from proposition [ prop : proper - clt ] , because @xmath34 is trivially uniformly ergodic and @xmath540 .\n",
      "note that @xmath541 implying @xmath542 .\n",
      "similarly , the convergence of @xmath543 follows from proposition [ prop : importance - var ] .\n",
      "let @xmath544 $ ] , then @xmath545 where @xmath546 $ ] .\n",
      "this implies that @xmath547    whenever @xmath548 , we may write @xmath354 = \\frac{n^{-1/2 }        \\sum_{k=1}^n w_k \\sum_{i=1}^n v_k^{(i ) }        \\bar{f}(x_k , z_k^{(i)})}{n^{-1 }        \\sum_{j=1}^n w_j}.      \\label{eq : clt - form}\\ ] ] the denominator converges to @xmath42 almost surely , so by slutsky s lemma it is enough to show that the numerator converges in distribution to @xmath549 .\n",
      "this follows from proposition [ prop : aug - clt ] , because @xmath291 .    for @xmath345 large enough such that @xmath550 for @xmath551 ,\n",
      "we may write @xmath552 the denominator converges to @xmath553 , and the numerator can be written as @xmath554,\\ ] ] and @xmath555 . the term @xmath556 , and because @xmath557 , the remainder terms @xmath558 and @xmath559 , as @xmath560 .\n",
      "the following proposition complements ( * ? ? ? * lemma 1 ) and @xcite , which are stated for more specific cases .\n",
      "[ prop : jump - properties ] suppose @xmath89 is a markov chain with non - degenerate transition probability @xmath471 and @xmath322 its jump chain with corresponding holding times @xmath561 ( definition [ def : jump ] ) .\n",
      "then , the following hold :    a.   [ item : jump - trans ] @xmath322 is a markov chain with transition probability @xmath562 b.   [ item : jump - joint - trans ] the holding times @xmath561 are conditionally independent given @xmath322 , and each @xmath333 has geometric distribution with parameter @xmath563 . c.   [ item : jump - invar ] if @xmath471 admits invariant probability @xmath564 , then @xmath565 admits the invariant probability @xmath566 in addition , if @xmath471 is reversible with respect to @xmath5 , then @xmath565 is reversible with respect to @xmath567 . d.   [ item : psi - irreducibility ] @xmath89 is @xmath568-irreducible if and only if @xmath322 is @xmath568-irreducible , with the same maximal irreducibility measure .\n",
      "e.   [ item : harris - recurrence ] @xmath89 is harris recurrent if and only if @xmath322 is harris recurrent .\n",
      "the expression of the transition probability is due to straightforward conditioning , and was observed in @xcite .\n",
      "the invariance follows from @xmath569 \\\\                & = \\frac{1}{\\mu(a)}\\bigg [ \\mu(a ) -\n",
      "\\int_a \\mu({\\mathrm{d}}x)\\big(1-a(x)\\big)\\bigg ]                = \\tilde{\\mu}(a ) ,            \\end{aligned}\\ ] ] and the reversibility is shown in @xcite .\n",
      "for it is sufficient to observe that @xmath570 which holds because the sets @xmath571 and @xmath572 coincide .\n",
      "similarly , holds because @xmath573 where @xmath574 and @xmath575 .          from the expression of the jump chain transition probability in proposition [ prop : jump - properties ]\n",
      ", we obtain for any @xmath582 with @xmath583 well - defined , @xmath584 consequently , we observe that @xmath585 implying . because @xmath579 , lemma [ lem : poisson - variance ] and a straightforward calculation yield @xmath586 ,            \\end{aligned}\\ ] ] where @xmath587 .\n",
      "similarly , by lemma [ lem : poisson - variance ] , @xmath588 which allows us to conclude .\n",
      "[ lem : poisson - variance ] suppose the markov transition probability @xmath471 on @xmath0 is harris ergodic with respect to @xmath5 and @xmath589 . if there exists @xmath509 which solves the poisson equation , @xmath590 then @xmath591 and the markov chain @xmath34 following @xmath471 satisfies a clt .\n",
      "let @xmath14 , @xmath592 and @xmath266 be the markov transition probabilities of @xmath34 , @xmath313 and @xmath593 , respectively , and let @xmath37 , @xmath594 and @xmath268 be the corresponding invariant probabilities .\n",
      "because @xmath34 and is @xmath288-geometric and @xmath595 , there exists a solution of poisson s equation @xmath596 such that @xmath597 .\n",
      "geometric ergodicity of @xmath34 implies also @xmath598 for @xmath37-almost every @xmath64 @xcite , so @xmath599 .\n",
      "proposition [ prop : jump - variance ] implies that @xmath600 , and @xmath601 .\n",
      "denote then @xmath602 which has conditional mean @xmath603 , and @xmath604 is finite because @xmath605 \\le \\frac{2-\\alpha(x)}{\\alpha^2(x ) }      m_\\xi^{(2)}(x).\\ ] ] lemma [ lem : aug - poisson ] therefore implies that there exists @xmath606 such that @xmath607 , and consequently lemma [ lem : poisson - variance ] implies that @xmath593 satisfies a clt for @xmath608 .    for the expression of the asymptotic variance , proposition [ prop : aug - asvar ] applied with @xmath609 , @xmath610 , @xmath611 and @xmath612\n",
      ", implies @xmath613 where , by the variance decomposition formula , @xmath614{\\mathrel{\\big|}}\\tilde{x}_k = x\\big ) \\\\      &\n",
      "= v_{n\\xi}(x ) + w_u^2(x ) \\bar{f}^{*2}(x )      \\big(1-\\alpha(x)\\big)/\\alpha^2(x).\\end{aligned}\\ ] ] proposition [ prop : jump - variance ] implies that @xmath615.\\ ] ] if we write the scaled estimator analogously as in , we notice that the clt holds for the numerator , with asymptotic variance @xmath616/\\pi_a(\\alpha).\\ ] ] the denominator converges to @xmath617 , so we conclude with slutsky s lemma .\n",
      "a.  beskos , o.  papaspiliopoulos , g.  o. roberts , and p.  fearnhead .\n",
      "exact and computationally efficient likelihood - based estimation for discretely observed diffusion processes .\n",
      ", 68(3):333382 , 2006 .\n",
      "( with discussion ) .\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.88 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.10 GiB is free. Including non-PyTorch memory, this process has 35.28 GiB memory in use. Of the allocated memory 33.89 GiB is allocated by PyTorch, and 907.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# summarize dialogue\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..................................................Summarize Text..................................\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflan-t5-base summary:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mres[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/pipelines/text2text_generation.py:269\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m          ids of the summary.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/pipelines/text2text_generation.py:167\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[1;32m    172\u001b[0m     ):\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/pipelines/text2text_generation.py:191\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     in_b, input_length \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_inputs(\n\u001b[1;32m    187\u001b[0m     input_length,\n\u001b[1;32m    188\u001b[0m     generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_length),\n\u001b[1;32m    189\u001b[0m     generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_length),\n\u001b[1;32m    190\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/generation/utils.py:1548\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1541\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1542\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1543\u001b[0m         )\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1548\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/generation/utils.py:661\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    659\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    660\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 661\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1113\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1099\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1100\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         output_attentions,\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:694\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    704\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:601\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    592\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    599\u001b[0m ):\n\u001b[1;32m    600\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 601\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    611\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:543\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    541\u001b[0m         position_bias\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 543\u001b[0m     position_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# if key and values are already calculated\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# we want only the last query position bias\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:443\u001b[0m, in \u001b[0;36mT5Attention.compute_bias\u001b[0;34m(self, query_length, key_length, device)\u001b[0m\n\u001b[1;32m    441\u001b[0m memory_position \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(key_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m    442\u001b[0m relative_position \u001b[38;5;241m=\u001b[39m memory_position \u001b[38;5;241m-\u001b[39m context_position  \u001b[38;5;66;03m# shape (query_length, key_length)\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m relative_position_bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_relative_position_bucket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shape (query_length, key_length)\u001b[39;49;00m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_decoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_buckets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelative_attention_num_buckets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelative_attention_max_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention_bias(relative_position_bucket)  \u001b[38;5;66;03m# shape (query_length, key_length, num_heads)\u001b[39;00m\n\u001b[1;32m    450\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape (1, num_heads, query_length, key_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/newgalaxie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:424\u001b[0m, in \u001b[0;36mT5Attention._relative_position_bucket\u001b[0;34m(relative_position, bidirectional, num_buckets, max_distance)\u001b[0m\n\u001b[1;32m    421\u001b[0m is_small \u001b[38;5;241m=\u001b[39m relative_position \u001b[38;5;241m<\u001b[39m max_exact\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m relative_position_if_large \u001b[38;5;241m=\u001b[39m max_exact \u001b[38;5;241m+\u001b[39m \u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelative_position\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_exact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_distance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_exact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_buckets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_exact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m relative_position_if_large \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(\n\u001b[1;32m    430\u001b[0m     relative_position_if_large, torch\u001b[38;5;241m.\u001b[39mfull_like(relative_position_if_large, num_buckets \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    431\u001b[0m )\n\u001b[1;32m    433\u001b[0m relative_buckets \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(is_small, relative_position, relative_position_if_large)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.88 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.10 GiB is free. Including non-PyTorch memory, this process has 35.28 GiB memory in use. Of the allocated memory 33.89 GiB is allocated by PyTorch, and 907.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from random import randrange\n",
    "\n",
    "# load model and tokenizer from huggingface hub with pipeline\n",
    "#summarizer = pipeline(\"summarization\", model=\"philschmid/flan-t5-base-samsum\", device=0)\n",
    "summarizer=pipeline('summarization',model=model,device='cuda',tokenizer=tokenizer)\n",
    "\n",
    "# select a random test sample\n",
    "#sample = db['test'][randrange(len(db[\"test\"]))]\n",
    "sample=db['test'][10]['article']\n",
    "print(f\"article: \\n{sample}\\n\\n\\n\")\n",
    "# summarize dialogue\n",
    "res = summarizer(sample)\n",
    "print(\"..................................................Summarize Text..................................\")\n",
    "print(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c569a-da00-4a4b-9a74-82ecc99b69b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newkernel",
   "language": "python",
   "name": "newgalaxie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
